"""
ä¼˜åŒ–ç‰ˆDMINTæ¨¡å‹ - ä¿®å¤å¤šæ ‡ç­¾åˆ†ç±»å’Œå…¬å¹³æ€§åˆ†ç±»é—®é¢˜
é’ˆå¯¹Psychological_fulfillmentæ ‡ç­¾æ— æ³•å­¦ä¹ å’Œfairnessæ€§èƒ½å·®çš„é—®é¢˜
"""

import pandas as pd
import json
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
from typing import Dict, List, Tuple
import time
from collections import Counter

# è®¾ç½®æ¨¡å‹è·¯å¾„
MODEL_PATH = "C:/Users/Administrator/Desktop/check919/models/chinese-roberta-wwm-ext"

class EnhancedTextDataset(Dataset):
    """å¢å¼ºç‰ˆæ–‡æœ¬æ•°æ®é›†ï¼Œæ·»åŠ æ•°æ®åˆ†æå’Œå¹³è¡¡å¤„ç†"""
    
    def __init__(self, dataframe, tokenizer, max_length=256, phase="train"):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.phase = phase
        
        # æ ‡ç­¾æ˜ å°„
        self.stance_map = {'Against': 0, 'Neither': 1, 'Favor': 2}
        self.fairness_map = {'Tinted': 0, 'Fairness': 1}
        self.harmfulness_map = {'Harmful': 0, 'Unharmful': 1}
        
        # Intentæ˜¯å¤šæ ‡ç­¾åˆ†ç±»
        self.intent_labels = ['Political_interest', 'Economic_interest', 
                             'Psychological_fulfillment', 'Public_interest']
        
        # åˆ†ææ•°æ®åˆ†å¸ƒ
        self.analyze_data_distribution()
        
    def analyze_data_distribution(self):
        """åˆ†ææ•°æ®åˆ†å¸ƒï¼Œè¯†åˆ«é—®é¢˜"""
        print(f"\n=== {self.phase}é›†æ•°æ®åˆ†å¸ƒåˆ†æ ===")
        
        # Stanceåˆ†å¸ƒ
        stance_counts = Counter(self.data['stance'])
        print(f"Stanceåˆ†å¸ƒ: {dict(stance_counts)}")
        
        # Intentåˆ†å¸ƒåˆ†æ
        intent_label_counts = [0] * len(self.intent_labels)
        intent_combinations = Counter()
        
        for idx, row in self.data.iterrows():
            intent_str = str(row['intent'])
            if pd.isna(intent_str):
                continue
                
            intent_list = intent_str.split('&')
            intent_combinations[intent_str] += 1
            
            for i, label in enumerate(self.intent_labels):
                if label in intent_list:
                    intent_label_counts[i] += 1
        
        print(f"Intentæ ‡ç­¾åˆ†å¸ƒ: {dict(zip(self.intent_labels, intent_label_counts))}")
        print(f"å‰5ä¸ªIntentç»„åˆ: {intent_combinations.most_common(5)}")
        
        # Fairnessåˆ†å¸ƒ
        fairness_counts = Counter(self.data['fairness'])
        print(f"Fairnessåˆ†å¸ƒ: {dict(fairness_counts)}")
        
        # Harmfulnessåˆ†å¸ƒ
        harmfulness_counts = Counter(self.data['harmfulness'])
        print(f"Harmfulnessåˆ†å¸ƒ: {dict(harmfulness_counts)}")
    
    def __len__(self):
        return len(self.data)
    
    def encode_intent(self, intent_str):
        """å°†å¤šæ ‡ç­¾intentç¼–ç ä¸ºäºŒè¿›åˆ¶å‘é‡"""
        if pd.isna(intent_str):
            return [0] * len(self.intent_labels)
        
        intent_list = str(intent_str).split('&')
        encoding = [0] * len(self.intent_labels)
        
        for i, label in enumerate(self.intent_labels):
            if label in intent_list:
                encoding[i] = 1
                
        return encoding
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        text = str(row['text']) if 'text' in row else "default text"
        
        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # è·å–æ ‡ç­¾ - æ·»åŠ å®¹é”™å¤„ç†
        stance = self.stance_map.get(str(row['stance']), 1)  # é»˜è®¤Neither
        fairness = self.fairness_map.get(str(row['fairness']), 0)  # é»˜è®¤Tinted
        harmfulness = self.harmfulness_map.get(str(row['harmfulness']), 0)  # é»˜è®¤Harmful
        intent = self.encode_intent(row['intent'])
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'stance': torch.tensor(stance, dtype=torch.long),
            'intent': torch.tensor(intent, dtype=torch.float),
            'fairness': torch.tensor(fairness, dtype=torch.long),
            'harmfulness': torch.tensor(harmfulness, dtype=torch.long)
        }

class MultiScaleFeatureExtractor(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾æå–å™¨ - æå–ä¸åŒç²’åº¦çš„æ–‡æœ¬ç‰¹å¾"""
    
    def __init__(self, input_dim: int = 768, output_dim: int = 256, dropout: float = 0.1):
        super(MultiScaleFeatureExtractor, self).__init__()
        
        # å¤šå°ºåº¦å·ç§¯å±‚
        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=2, padding=1)  # å±€éƒ¨ç‰¹å¾
        self.conv2 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)  # ä¸­ç­‰ç‰¹å¾
        self.conv3 = nn.Conv1d(input_dim, 128, kernel_size=5, padding=2)  # å…¨å±€ç‰¹å¾
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(384, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, output_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # è½¬ç½®ä¸ºå·ç§¯éœ€è¦çš„æ ¼å¼ [batch_size, hidden_size, seq_len]
        x = x.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯
        local_feat = F.relu(self.conv1(x))
        medium_feat = F.relu(self.conv2(x))
        global_feat = F.relu(self.conv3(x))
        
        # å…¨å±€å¹³å‡æ± åŒ–
        local_pool = F.adaptive_avg_pool1d(local_feat, 1).squeeze(-1)
        medium_pool = F.adaptive_avg_pool1d(medium_feat, 1).squeeze(-1)
        global_pool = F.adaptive_avg_pool1d(global_feat, 1).squeeze(-1)
        
        # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾
        multi_scale_features = torch.cat([local_pool, medium_pool, global_pool], dim=1)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion(multi_scale_features)
        fused_features = self.layer_norm(fused_features)
        
        return fused_features

class DifferentiatedFeatureExtractor(nn.Module):
    """å·®å¼‚åŒ–å¤šè§†è§’ç‰¹å¾æå–å™¨ - æ¯ä¸ªè§†è§’æœ‰ä¸åŒçš„ç½‘ç»œç»“æ„"""
    
    def __init__(self, perspective_type: str, input_dim: int = 768, output_dim: int = 256, dropout: float = 0.1):
        super(DifferentiatedFeatureExtractor, self).__init__()
        self.perspective_type = perspective_type
        
        if perspective_type == "belief":
            # ä¿¡å¿µè§†è§’ï¼šæ›´æ·±ç½‘ç»œï¼Œå…³æ³¨æ·±å±‚æ¬¡ç†è§£
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 512),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(512, 384),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(384, output_dim),
                nn.LayerNorm(output_dim)
            )
        elif perspective_type == "desire":
            # æ¬²æœ›è§†è§’ï¼šä¸­ç­‰æ·±åº¦ï¼Œå…³æ³¨åŠ¨æœº
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 448),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(448, output_dim),
                nn.LayerNorm(output_dim)
            )
        else:  # plan
            # è®¡åˆ’è§†è§’ï¼šæ›´æµ…ç½‘ç»œï¼Œå…³æ³¨è¡ŒåŠ¨å±‚é¢
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 320),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(320, output_dim),
                nn.LayerNorm(output_dim)
            )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.feature_net(x)

class OptimizedIntentClassifier(nn.Module):
    """ä¼˜åŒ–ç‰ˆæ„å›¾åˆ†ç±»å™¨ - ä¸“é—¨è§£å†³Psychological_fulfillmentæ— æ³•å­¦ä¹ çš„é—®é¢˜"""
    
    def __init__(self, feature_dim: int = 256, num_intents: int = 4, dropout: float = 0.2):
        super(OptimizedIntentClassifier, self).__init__()
        
        # ä¸ºæ¯ä¸ªæ„å›¾æ ‡ç­¾åˆ›å»ºç‹¬ç«‹çš„åˆ†ç±»è·¯å¾„
        self.intent_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(feature_dim, 128),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(128, 64),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(64, 1)  # æ¯ä¸ªæ ‡ç­¾å•ç‹¬è¾“å‡º
            ) for _ in range(num_intents)
        ])
        
        # æ ‡ç­¾é—´å…³ç³»å»ºæ¨¡
        self.label_correlation = nn.MultiheadAttention(
            embed_dim=num_intents,
            num_heads=2,
            dropout=dropout,
            batch_first=True
        )
        
        self.num_intents = num_intents
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)
        
        # æ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹é¢„æµ‹
        intent_logits_list = []
        for i, head in enumerate(self.intent_heads):
            logit = head(x)  # [batch_size, 1]
            intent_logits_list.append(logit)
        
        # æ‹¼æ¥æ‰€æœ‰æ ‡ç­¾çš„logits
        intent_logits = torch.cat(intent_logits_list, dim=1)  # [batch_size, num_intents]
        
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡æ ‡ç­¾é—´å…³ç³»
        intent_logits_reshaped = intent_logits.unsqueeze(1)  # [batch_size, 1, num_intents]
        correlated_logits, _ = self.label_correlation(
            intent_logits_reshaped, 
            intent_logits_reshaped, 
            intent_logits_reshaped
        )
        correlated_logits = correlated_logits.squeeze(1)  # [batch_size, num_intents]
        
        # æ®‹å·®è¿æ¥
        final_logits = intent_logits + 0.3 * correlated_logits
        
        return final_logits

class EnhancedFairnessClassifier(nn.Module):
    """å¢å¼ºç‰ˆå…¬å¹³æ€§åˆ†ç±»å™¨ - ä¸“é—¨ä¼˜åŒ–fairnessåˆ†ç±»"""
    
    def __init__(self, feature_dim: int = 256, dropout: float = 0.2):
        super(EnhancedFairnessClassifier, self).__init__()
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„æ¥æ•æ‰å…¬å¹³æ€§ç‰¹å¾
        self.fairness_net = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(64, 2)  # Tinted, Fairness
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶èšç„¦å…³é”®ç‰¹å¾
        self.attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾
        x_reshaped = x.unsqueeze(1)  # [batch_size, 1, feature_dim]
        attended_x, _ = self.attention(x_reshaped, x_reshaped, x_reshaped)
        attended_x = attended_x.squeeze(1)
        
        # æ®‹å·®è¿æ¥
        enhanced_features = x + 0.2 * attended_x
        
        # åˆ†ç±»
        fairness_logits = self.fairness_net(enhanced_features)
        
        return fairness_logits

class GatedIntentAggregator(nn.Module):
    """é—¨æ§æ„å›¾èšåˆå™¨ - åŠ¨æ€èåˆä¸åŒè§†è§’çš„ç‰¹å¾"""
    
    def __init__(self, feature_dim: int = 256, num_intents: int = 4, dropout: float = 0.1):
        super(GatedIntentAggregator, self).__init__()
        
        # é—¨æ§æœºåˆ¶ - å­¦ä¹ æ¯ä¸ªè§†è§’çš„é‡è¦æ€§æƒé‡
        self.gate_network = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, 3),  # 3ä¸ªè§†è§’çš„æƒé‡
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion_network = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆçš„æ„å›¾åˆ†ç±»å™¨
        self.intent_classifier = OptimizedIntentClassifier(feature_dim, num_intents, dropout)
        
    def forward(self, belief_features: torch.Tensor, desire_features: torch.Tensor, 
                plan_features: torch.Tensor) -> Dict[str, torch.Tensor]:
        
        batch_size = belief_features.size(0)
        
        # æ‹¼æ¥æ‰€æœ‰è§†è§’ç‰¹å¾
        concatenated = torch.cat([belief_features, desire_features, plan_features], dim=1)
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate_weights = self.gate_network(concatenated)
        
        # åº”ç”¨é—¨æ§æƒé‡çš„åŠ æƒèåˆ
        weighted_belief = belief_features * gate_weights[:, 0:1]
        weighted_desire = desire_features * gate_weights[:, 1:2]
        weighted_plan = plan_features * gate_weights[:, 2:3]
        
        # æ‹¼æ¥åŠ æƒç‰¹å¾
        weighted_concatenated = torch.cat([weighted_belief, weighted_desire, weighted_plan], dim=1)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion_network(weighted_concatenated)
        
        # æ„å›¾åˆ†ç±»
        intent_logits = self.intent_classifier(fused_features)
        
        return {
            'intent_logits': intent_logits,
            'fused_features': fused_features,
            'gate_weights': gate_weights
        }

class OptimizedDMINTModel(nn.Module):
    """ä¼˜åŒ–ç‰ˆDMINTæ¨¡å‹ - ä¸“é—¨è§£å†³å¤šæ ‡ç­¾åˆ†ç±»å’Œå…¬å¹³æ€§åˆ†ç±»é—®é¢˜"""
    
    def __init__(self, bert_model, hidden_size: int = 768, feature_dim: int = 256, dropout: float = 0.3):
        super(OptimizedDMINTModel, self).__init__()
        
        self.bert = bert_model
        
        # å¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼ˆç”¨äºåºåˆ—ç‰¹å¾ï¼‰
        self.multi_scale_extractor = MultiScaleFeatureExtractor(hidden_size, feature_dim, dropout)
        
        # ä¸‰ä¸ªå·®å¼‚åŒ–çš„è§†è§’ç‰¹å¾æå–å™¨ï¼ˆç”¨äºCLSç‰¹å¾ï¼‰
        self.belief_extractor = DifferentiatedFeatureExtractor("belief", hidden_size, feature_dim, dropout)
        self.desire_extractor = DifferentiatedFeatureExtractor("desire", hidden_size, feature_dim, dropout)
        self.plan_extractor = DifferentiatedFeatureExtractor("plan", hidden_size, feature_dim, dropout)
        
        # é—¨æ§æ„å›¾èšåˆå™¨
        self.intent_aggregator = GatedIntentAggregator(feature_dim, 4, dropout)
        
        # ä»»åŠ¡ç‰¹å®šçš„åˆ†ç±»å™¨ - ä½¿ç”¨å¢å¼ºç‰ˆ
        self.stance_classifier = nn.Sequential(
            nn.Linear(feature_dim * 2, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3)  # Against, Neither, Favor
        )
        
        self.harmfulness_classifier = nn.Sequential(
            nn.Linear(feature_dim * 2, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)  # Harmful, Unharmful
        )
        
        # ä½¿ç”¨å¢å¼ºç‰ˆå…¬å¹³æ€§åˆ†ç±»å™¨
        self.fairness_classifier = EnhancedFairnessClassifier(feature_dim, dropout)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        # BERTç‰¹å¾æå–
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = bert_output.last_hidden_state
        cls_features = bert_output.pooler_output
        
        # å¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆåŸºäºåºåˆ—è¾“å‡ºï¼‰
        multi_scale_features = self.multi_scale_extractor(sequence_output)
        
        # ä¸‰ä¸ªè§†è§’çš„ç‰¹å¾æå–ï¼ˆåŸºäºCLSç‰¹å¾ï¼‰
        belief_features = self.belief_extractor(cls_features)
        desire_features = self.desire_extractor(cls_features)
        plan_features = self.plan_extractor(cls_features)
        
        # åº”ç”¨dropout
        belief_features = self.dropout(belief_features)
        desire_features = self.dropout(desire_features)
        plan_features = self.dropout(plan_features)
        multi_scale_features = self.dropout(multi_scale_features)
        
        # é—¨æ§æ„å›¾èšåˆ
        intent_output = self.intent_aggregator(belief_features, desire_features, plan_features)
        fused_features = intent_output['fused_features']
        intent_logits = intent_output['intent_logits']
        
        # ä»»åŠ¡åˆ†ç±» - ç»“åˆå¤šå°ºåº¦ç‰¹å¾å’Œè§†è§’ç‰¹å¾
        stance_input = torch.cat([multi_scale_features, belief_features], dim=1)
        stance_logits = self.stance_classifier(stance_input)
        
        harmfulness_input = torch.cat([multi_scale_features, plan_features], dim=1)
        harmfulness_logits = self.harmfulness_classifier(harmfulness_input)
        
        fairness_logits = self.fairness_classifier(fused_features)
        
        return {
            'stance': stance_logits,
            'intent': intent_logits,
            'harmfulness': harmfulness_logits,
            'fairness': fairness_logits,
            'belief_features': belief_features,
            'desire_features': desire_features,
            'plan_features': plan_features,
            'multi_scale_features': multi_scale_features,
            'gate_weights': intent_output['gate_weights']
        }

def compute_balanced_class_weights(train_df):
    """è®¡ç®—å¹³è¡¡çš„ç±»åˆ«æƒé‡ï¼Œç‰¹åˆ«å¤„ç†ç¨€æœ‰æ ‡ç­¾"""
    intent_labels = ['Political_interest', 'Economic_interest', 
                    'Psychological_fulfillment', 'Public_interest']
    
    # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾çš„å‡ºç°æ¬¡æ•°
    label_counts = [0] * len(intent_labels)
    total_samples = len(train_df)
    
    for idx, row in train_df.iterrows():
        intent_str = str(row['intent'])
        if pd.isna(intent_str):
            continue
            
        intent_list = intent_str.split('&')
        for i, label in enumerate(intent_labels):
            if label in intent_list:
                label_counts[i] += 1
    
    print(f"Intentæ ‡ç­¾åˆ†å¸ƒ: {dict(zip(intent_labels, label_counts))}")
    
    # ç‰¹åˆ«å¤„ç†Psychological_fulfillment
    psych_idx = intent_labels.index('Psychological_fulfillment')
    if label_counts[psych_idx] < 500:  # å¦‚æœæ ·æœ¬è¿‡å°‘
        print(f"è­¦å‘Š: Psychological_fulfillmentæ ·æœ¬è¿‡å°‘({label_counts[psych_idx]})ï¼Œåº”ç”¨ç‰¹æ®Šæƒé‡")
        # ç»™ç¨€æœ‰æ ‡ç­¾æ›´é«˜æƒé‡
        base_weights = [total_samples / (len(intent_labels) * count) if count > 0 else 1.0 
                       for count in label_counts]
        base_weights[psych_idx] *= 2.0  # åŒå€æƒé‡
    else:
        base_weights = [total_samples / (len(intent_labels) * count) if count > 0 else 1.0 
                       for count in label_counts]
    
    print(f"å¹³è¡¡åIntentç±»åˆ«æƒé‡: {dict(zip(intent_labels, [f'{w:.2f}' for w in base_weights]))}")
    
    return torch.tensor(base_weights, dtype=torch.float)

def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    data_dir = "data/"
    
    # åŠ è½½CSVæ•°æ®
    train_df = pd.read_csv(os.path.join(data_dir, "train.csv"), sep=',', header=None,
                          names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    val_df = pd.read_csv(os.path.join(data_dir, "val.csv"), sep=',', header=None,
                        names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    test_df = pd.read_csv(os.path.join(data_dir, "test1.csv"), sep=',', header=None,
                         names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    
    # åŠ è½½JSONæ•°æ®å¹¶æ•´åˆæ–‡æœ¬
    def load_json_data():
        topics_path = os.path.join(data_dir, "news_topic1.json")
        docs_path = os.path.join(data_dir, "news_docs.json")
        
        topics_data = {}
        docs_data = {}
        
        if os.path.exists(topics_path):
            with open(topics_path, 'r', encoding='utf-8') as f:
                topics_data = json.load(f)
        
        if os.path.exists(docs_path):
            with open(docs_path, 'r', encoding='utf-8') as f:
                docs_data = json.load(f)
        
        return topics_data, docs_data
    
    topics_data, docs_data = load_json_data()
    
    def get_text_content(row_id):
        """è·å–æ–‡æœ¬å†…å®¹"""
        str_id = str(row_id)
        topic_text = topics_data.get(str_id, "unknown topic")
        
        # å¦‚æœæœ‰æ–‡æ¡£æ•°æ®ï¼Œä½¿ç”¨æ–‡æ¡£å†…å®¹ï¼›å¦åˆ™åªä½¿ç”¨æ ‡é¢˜
        doc_content = ""
        if docs_data and str_id in docs_data:
            doc_content = docs_data.get(str_id, {}).get("content", "")
        
        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè‡³å°‘ä½¿ç”¨æ ‡é¢˜
        if not doc_content.strip():
            return topic_text
        else:
            return f"{topic_text} {doc_content}"
    
    # ä¸ºæ•°æ®æ¡†æ·»åŠ æ–‡æœ¬åˆ—
    train_df['text'] = train_df['id'].apply(get_text_content)
    val_df['text'] = val_df['id'].apply(get_text_content)
    test_df['text'] = test_df['id'].apply(get_text_content)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_df)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")
    
    return train_df, val_df, test_df

def adaptive_intent_prediction(intent_logits, intent_labels, method='adaptive_threshold'):
    """è‡ªé€‚åº”å¤šæ ‡ç­¾é¢„æµ‹ç­–ç•¥"""
    intent_probs = torch.sigmoid(intent_logits)
    
    if method == 'adaptive_threshold':
        # è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥
        batch_size = intent_probs.size(0)
        predictions = []
        
        for i in range(batch_size):
            # è®¡ç®—æ ·æœ¬çº§åˆ«çš„é˜ˆå€¼
            sample_probs = intent_probs[i]
            sample_labels = intent_labels[i]
            
            # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼ŒåŸºäºçœŸå®æ ‡ç­¾æ•°é‡è°ƒæ•´é˜ˆå€¼
            num_true_labels = sample_labels.sum().item()
            if num_true_labels > 0:
                # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„kä¸ªæ ‡ç­¾
                k = max(1, int(num_true_labels))
                _, topk_indices = torch.topk(sample_probs, k)
                pred = torch.zeros_like(sample_probs)
                pred[topk_indices] = 1
            else:
                # ä½¿ç”¨å›ºå®šé˜ˆå€¼
                pred = (sample_probs > 0.4).float()
            
            predictions.append(pred.cpu().numpy())
        
        return np.array(predictions)
    
    elif method == 'label_specific_threshold':
        # æ ‡ç­¾ç‰¹å®šé˜ˆå€¼
        thresholds = [0.3, 0.4, 0.2, 0.4]  # ä¸ºæ¯ä¸ªæ ‡ç­¾è®¾ç½®ä¸åŒé˜ˆå€¼
        predictions = []
        
        for i in range(intent_probs.size(0)):
            sample_pred = []
            for j in range(intent_probs.size(1)):
                if intent_probs[i, j] > thresholds[j]:
                    sample_pred.append(1)
                else:
                    sample_pred.append(0)
            predictions.append(sample_pred)
        
        return np.array(predictions)
    
    else:
        # é»˜è®¤å›ºå®šé˜ˆå€¼
        return (intent_probs > 0.3).float().cpu().numpy()

def safe_intent_metrics(predictions, labels):
    """å®‰å…¨åœ°è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡ï¼Œå¤„ç†ç»´åº¦é—®é¢˜"""
    if len(predictions) == 0 or len(labels) == 0:
        return {
            'exact_match': 0.0,
            'macro_f1': 0.0,
            'micro_f1': 0.0,
            'per_label_f1': [0.0, 0.0, 0.0, 0.0]
        }
    
    try:
        predictions_array = np.array(predictions)
        labels_array = np.array(labels)
        
        # ç¡®ä¿æ•°ç»„æ˜¯äºŒç»´çš„
        if predictions_array.ndim == 1:
            predictions_array = predictions_array.reshape(1, -1)
            labels_array = labels_array.reshape(1, -1)
        
        # ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡
        exact_match = np.mean(np.all(predictions_array == labels_array, axis=1))
        
        # è®¡ç®—F1åˆ†æ•°
        try:
            macro_f1 = f1_score(labels_array, predictions_array, average='macro', zero_division=0)
            micro_f1 = f1_score(labels_array, predictions_array, average='micro', zero_division=0)
            per_label_f1 = f1_score(labels_array, predictions_array, average=None, zero_division=0)
        except:
            macro_f1 = 0.0
            micro_f1 = 0.0
            per_label_f1 = [0.0] * 4
        
        return {
            'exact_match': float(exact_match),
            'macro_f1': float(macro_f1),
            'micro_f1': float(micro_f1),
            'per_label_f1': per_label_f1.tolist() if hasattr(per_label_f1, 'tolist') else per_label_f1
        }
    except Exception as e:
        print(f"è®¡ç®—intentæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return {
            'exact_match': 0.0,
            'macro_f1': 0.0,
            'micro_f1': 0.0,
            'per_label_f1': [0.0, 0.0, 0.0, 0.0]
        }

class OptimizedDMINTTrainer:
    """ä¼˜åŒ–ç‰ˆè®­ç»ƒå™¨ - ä¸“é—¨è§£å†³å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜"""
    
    def __init__(self, model, train_loader, val_loader, device, num_epochs=5, intent_class_weights=None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.num_epochs = num_epochs
        
        # æŸå¤±å‡½æ•° - ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨å¸¦æƒé‡çš„BCE
        self.criterion_stance = nn.CrossEntropyLoss()
        
        # ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨å¸¦æƒé‡çš„BCEWithLogitsLoss
        if intent_class_weights is not None:
            self.criterion_intent = nn.BCEWithLogitsLoss(pos_weight=intent_class_weights.to(device))
            print(f"ä½¿ç”¨åŠ æƒçš„å¤šæ ‡ç­¾æŸå¤±å‡½æ•°ï¼Œæƒé‡: {intent_class_weights}")
        else:
            self.criterion_intent = nn.BCEWithLogitsLoss()
            
        self.criterion_harmfulness = nn.CrossEntropyLoss()
        self.criterion_fairness = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=1e-5,  # é™ä½å­¦ä¹ ç‡
            weight_decay=0.01
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=10,
            T_mult=2
        )
        
        self.best_val_loss = float('inf')
        self.best_intent_f1 = 0.0
        self.best_fairness_acc = 0.0
        self.patience = 6
        self.counter = 0
        self.start_time = time.time()
        
    def compute_balanced_loss(self, outputs, labels):
        """è®¡ç®—å¹³è¡¡çš„å¤šä»»åŠ¡æŸå¤±"""
        loss_stance = self.criterion_stance(outputs['stance'], labels['stance'])
        loss_intent = self.criterion_intent(outputs['intent'], labels['intent'])
        loss_harmfulness = self.criterion_harmfulness(outputs['harmfulness'], labels['harmfulness'])
        loss_fairness = self.criterion_fairness(outputs['fairness'], labels['fairness'])
        
        # å¹³è¡¡çš„æƒé‡åˆ†é…
        total_loss = (loss_stance + 2.0 * loss_intent +  # é™ä½intentæƒé‡
                     loss_harmfulness + 1.5 * loss_fairness)  # æé«˜fairnessæƒé‡
        
        return total_loss, {
            'stance': loss_stance.item(),
            'intent': loss_intent.item(),
            'harmfulness': loss_harmfulness.item(),
            'fairness': loss_fairness.item()
        }
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_batches = len(self.train_loader)
        
        epoch_start_time = time.time()
        
        for batch_idx, batch in enumerate(self.train_loader):
            batch_start_time = time.time()
            
            self.optimizer.zero_grad()
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            stance_labels = batch['stance'].to(self.device)
            intent_labels = batch['intent'].to(self.device)
            harmfulness_labels = batch['harmfulness'].to(self.device)
            fairness_labels = batch['fairness'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids, attention_mask)
            
            # è®¡ç®—å¹³è¡¡çš„å¤šä»»åŠ¡æŸå¤±
            labels = {
                'stance': stance_labels,
                'intent': intent_labels,
                'harmfulness': harmfulness_labels,
                'fairness': fairness_labels
            }
            total_loss_batch, task_losses = self.compute_balanced_loss(outputs, labels)
            
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            
            # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if batch_idx % 10 == 0:
                progress = (batch_idx / total_batches) * 100
                batch_time = time.time() - batch_start_time
                
                print(f'  Batch {batch_idx}/{total_batches} ({progress:.1f}%), '
                      f'Total Loss: {total_loss_batch.item():.4f}, '
                      f'Intent Loss: {task_losses["intent"]:.4f}, '
                      f'Fairness Loss: {task_losses["fairness"]:.4f}, '
                      f'Batch Time: {batch_time:.2f}s')
        
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / total_batches
        return avg_loss, epoch_time
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        total_batches = len(self.val_loader)
        
        all_predictions = {'stance': [], 'intent': [], 'harmfulness': [], 'fairness': []}
        all_labels = {'stance': [], 'intent': [], 'harmfulness': [], 'fairness': []}
        
        with torch.no_grad():
            for batch in self.val_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                stance_labels = batch['stance'].to(self.device)
                intent_labels = batch['intent'].to(self.device)
                harmfulness_labels = batch['harmfulness'].to(self.device)
                fairness_labels = batch['fairness'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids, attention_mask)
                
                # è®¡ç®—æŸå¤±
                labels = {
                    'stance': stance_labels,
                    'intent': intent_labels,
                    'harmfulness': harmfulness_labels,
                    'fairness': fairness_labels
                }
                total_loss_batch, _ = self.compute_balanced_loss(outputs, labels)
                total_loss += total_loss_batch.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions['stance'].extend(torch.argmax(outputs['stance'], 1).cpu().numpy())
                all_labels['stance'].extend(stance_labels.cpu().numpy())
                
                all_predictions['harmfulness'].extend(torch.argmax(outputs['harmfulness'], 1).cpu().numpy())
                all_labels['harmfulness'].extend(harmfulness_labels.cpu().numpy())
                
                all_predictions['fairness'].extend(torch.argmax(outputs['fairness'], 1).cpu().numpy())
                all_labels['fairness'].extend(fairness_labels.cpu().numpy())
                
                # ä½¿ç”¨è‡ªé€‚åº”å¤šæ ‡ç­¾é¢„æµ‹
                intent_predictions = adaptive_intent_prediction(
                    outputs['intent'], intent_labels, method='adaptive_threshold'
                )
                all_predictions['intent'].extend(intent_predictions)
                all_labels['intent'].extend(intent_labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for task in ['stance', 'harmfulness', 'fairness']:
            if len(all_labels[task]) > 0:
                metrics[f'{task}_accuracy'] = accuracy_score(all_labels[task], all_predictions[task])
                metrics[f'{task}_f1'] = f1_score(all_labels[task], all_predictions[task], average='weighted')
            else:
                metrics[f'{task}_accuracy'] = 0.0
                metrics[f'{task}_f1'] = 0.0
        
        # Intentçš„å¤šæ ‡ç­¾æŒ‡æ ‡
        intent_metrics = safe_intent_metrics(all_predictions['intent'], all_labels['intent'])
        metrics['intent_exact_match'] = intent_metrics['exact_match']
        metrics['intent_macro_f1'] = intent_metrics['macro_f1']
        metrics['intent_micro_f1'] = intent_metrics['micro_f1']
        
        # æ¯ä¸ªintentæ ‡ç­¾çš„F1
        intent_labels_names = ['Political', 'Economic', 'Psychological', 'Public']
        for i, label in enumerate(intent_labels_names):
            metrics[f'intent_{label}_f1'] = intent_metrics['per_label_f1'][i] if i < len(intent_metrics['per_label_f1']) else 0.0
        
        avg_loss = total_loss / total_batches
        return avg_loss, metrics
        
    def train(self):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
        total_start_time = time.time()
        
        for epoch in range(self.num_epochs):
            print(f"\nEpoch {epoch+1}/{self.num_epochs}")
            print("-" * 50)
            
            train_loss, epoch_time = self.train_epoch(epoch + 1)
            val_loss, val_metrics = self.validate()
            
            self.scheduler.step()
            
            # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
            total_time = time.time() - total_start_time
            hours = int(total_time // 3600)
            minutes = int((total_time % 3600) // 60)
            
            print(f"\nEpoch {epoch+1} å®Œæˆ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  Epochæ—¶é—´: {epoch_time/60:.1f}åˆ†é’Ÿ")
            print(f"  æ€»è®­ç»ƒæ—¶é—´: {hours}å°æ—¶{minutes}åˆ†é’Ÿ")
            print(f"  éªŒè¯æŒ‡æ ‡:")
            for metric, value in val_metrics.items():
                print(f"    {metric}: {value:.4f}")
            
            # æ”¹è¿›çš„æ—©åœæœºåˆ¶ - åŸºäºå¤šä¸ªæŒ‡æ ‡
            current_intent_f1 = val_metrics['intent_macro_f1']
            current_fairness_acc = val_metrics['fairness_accuracy']
            current_psych_f1 = val_metrics['intent_Psychological_f1']
            
            # æ£€æŸ¥Psychological_fulfillmentæ˜¯å¦å¼€å§‹å­¦ä¹ 
            psych_improved = current_psych_f1 > 0.0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹çš„æ¡ä»¶æ›´å®½æ¾
            improvement_threshold = 0.005
            
            if (current_intent_f1 > self.best_intent_f1 + improvement_threshold or
                current_fairness_acc > self.best_fairness_acc + improvement_threshold or
                psych_improved):
                
                if current_intent_f1 > self.best_intent_f1:
                    self.best_intent_f1 = current_intent_f1
                if current_fairness_acc > self.best_fairness_acc:
                    self.best_fairness_acc = current_fairness_acc
                    
                torch.save(self.model.state_dict(), 'outputs/optimized_dmint_model.pth')
                print("âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹! (åŸºäºç»¼åˆæŒ‡æ ‡æ”¹è¿›)")
                self.counter = 0
                
                if psych_improved:
                    print(f"ğŸ‰ Psychological_fulfillmentå¼€å§‹å­¦ä¹ ! F1: {current_psych_f1:.4f}")
            else:
                self.counter += 1
                if self.counter >= self.patience:
                    print(f"âš  æ—©åœ: {self.patience} ä¸ªepochæœªæ˜¾è‘—æå‡")
                    break
            
            print("=" * 60)

def main_training():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
        bert_model = BertModel.from_pretrained(MODEL_PATH)
        print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # åŠ è½½æ•°æ®
        train_df, val_df, test_df = load_and_preprocess_data()
        
        # åˆ›å»ºå¢å¼ºç‰ˆæ•°æ®é›†ï¼ˆåŒ…å«æ•°æ®åˆ†æï¼‰
        train_dataset = EnhancedTextDataset(train_df, tokenizer, phase="è®­ç»ƒ")
        val_dataset = EnhancedTextDataset(val_df, tokenizer, phase="éªŒè¯")
        test_dataset = EnhancedTextDataset(test_df, tokenizer, phase="æµ‹è¯•")
        
        # è®¡ç®—å¹³è¡¡çš„ç±»åˆ«æƒé‡
        intent_class_weights = compute_balanced_class_weights(train_df)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)
        
        print(f"è®­ç»ƒé›†batchæ•°é‡: {len(train_loader)}")
        print(f"éªŒè¯é›†batchæ•°é‡: {len(val_loader)}")
        print(f"æµ‹è¯•é›†batchæ•°é‡: {len(test_loader)}")
        
        # åˆ›å»ºä¼˜åŒ–ç‰ˆDMINTæ¨¡å‹
        model = OptimizedDMINTModel(bert_model)
        model = model.to(device)
        print("ä¼˜åŒ–ç‰ˆDMINTæ¨¡å‹åˆ›å»ºæˆåŠŸ!")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs('outputs', exist_ok=True)
        
        # è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨å¹³è¡¡çš„ç±»åˆ«æƒé‡
        trainer = OptimizedDMINTTrainer(
            model, train_loader, val_loader, device, 
            num_epochs=8,  # å¢åŠ è®­ç»ƒè½®æ•°
            intent_class_weights=intent_class_weights
        )
        trainer.train()
        
        print("\nè®­ç»ƒå®Œæˆ! ä¼˜åŒ–ç‰ˆæ¨¡å‹å·²ä¿å­˜åˆ° outputs/optimized_dmint_model.pth")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main_training()