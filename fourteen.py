"""
Fourteen.py - å¢å¼ºç‰ˆDMINTæ¨¡å‹ï¼Œèåˆè¯„è®ºæ•°æ®
åœ¨Thirteenç‰ˆåŸºç¡€ä¸Šå¢åŠ è¯„è®ºæ•°æ®ï¼Œä»æ•°æ®ç§ç±»å±‚é¢æé«˜è®­ç»ƒæ•ˆæœ
"""

import pandas as pd
import json
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
from typing import Dict, List, Tuple
import time
from collections import Counter, deque
import math

# è®¾ç½®æ¨¡å‹è·¯å¾„
MODEL_PATH = "models\chinese-roberta-wwm-ext"

class WarmupPolyLR:
    """Warmup + å¤šé¡¹å¼è¡°å‡å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, total_epochs, warmup_epochs=2, poly_exp=0.9, min_lr=1e-6, base_lr=2e-5):
        self.optimizer = optimizer
        self.total_epochs = total_epochs
        self.warmup_epochs = warmup_epochs
        self.poly_exp = poly_exp
        self.min_lr = min_lr
        self.base_lr = base_lr
        self.current_epoch = 0
        
    def step(self):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self.current_epoch += 1
        
        if self.current_epoch <= self.warmup_epochs:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢åŠ 
            lr_scale = self.current_epoch / self.warmup_epochs
            new_lr = self.base_lr * lr_scale
        else:
            # å¤šé¡¹å¼è¡°å‡
            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr_scale = (1.0 - progress) ** self.poly_exp
            new_lr = max(self.base_lr * lr_scale, self.min_lr)
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr
            
        return new_lr

class EnhancedTextDataset(Dataset):
    """å¢å¼ºç‰ˆæ–‡æœ¬æ•°æ®é›†ï¼Œæ·»åŠ è¯„è®ºæ•°æ®å’Œå¹³è¡¡å¤„ç†"""
    
    def __init__(self, dataframe, tokenizer, max_length=256, phase="train"):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.phase = phase
        
        # æ ‡ç­¾æ˜ å°„
        self.stance_map = {'Against': 0, 'Neither': 1, 'Favor': 2}
        self.fairness_map = {'Tinted': 0, 'Fairness': 1}
        self.harmfulness_map = {'Harmful': 0, 'Unharmful': 1}
        
        # Intentæ˜¯å¤šæ ‡ç­¾åˆ†ç±»
        self.intent_labels = ['Political_interest', 'Economic_interest', 
                             'Psychological_fulfillment', 'Public_interest']
        
        # åŠ è½½è¯„è®ºæ•°æ®
        self.id_post_mapping, self.post_data = self.load_comment_data()
        
        # åˆ†ææ•°æ®åˆ†å¸ƒ
        self.analyze_data_distribution()
        
    def load_comment_data(self):
        """åŠ è½½è¯„è®ºç›¸å…³æ•°æ®"""
        data_dir = "data/"
        
        # åŠ è½½id_post_pairæ˜ å°„
        id_post_path = os.path.join(data_dir, "id_post_pair.json")
        id_post_mapping = {}
        if os.path.exists(id_post_path):
            with open(id_post_path, 'r', encoding='utf-8') as f:
                id_post_mapping = json.load(f)
            print(f"åŠ è½½äº† {len(id_post_mapping)} ä¸ªIDåˆ°å¸–å­çš„æ˜ å°„")
        
        # åŠ è½½å¸–å­æ•°æ®
        post_path = os.path.join(data_dir, "post.json")
        post_data = {}
        if os.path.exists(post_path):
            with open(post_path, 'r', encoding='utf-8') as f:
                posts = json.load(f)
                for post in posts:
                    post_data[post['pid']] = post
            print(f"åŠ è½½äº† {len(post_data)} ä¸ªå¸–å­æ•°æ®")
        
        # åŠ è½½å¸–å­æ–‡æ¡£æ•°æ®ï¼ˆå¤‡ç”¨ï¼‰
        post_docs_path = os.path.join(data_dir, "post_docs.json")
        post_docs_data = {}
        if os.path.exists(post_docs_path):
            with open(post_docs_path, 'r', encoding='utf-8') as f:
                post_docs = json.load(f)
                for doc in post_docs:
                    post_docs_data[doc['post_id']] = doc
            print(f"åŠ è½½äº† {len(post_docs_data)} ä¸ªå¸–å­æ–‡æ¡£æ•°æ®")
        
        return id_post_mapping, post_data
    
    def get_comment_tree_text(self, post_id):
        """è·å–å¸–å­åŠå…¶è¯„è®ºæ ‘çš„å®Œæ•´æ–‡æœ¬"""
        if post_id not in self.post_data:
            return ""
        
        # ä½¿ç”¨BFSéå†è¯„è®ºæ ‘
        all_texts = []
        queue = deque([post_id])
        
        while queue:
            current_pid = queue.popleft()
            if current_pid in self.post_data:
                post = self.post_data[current_pid]
                # æ·»åŠ å½“å‰å¸–å­/è¯„è®ºçš„æ–‡æœ¬
                if 'content' in post and post['content']:
                    all_texts.append(post['content'])
                
                # æ·»åŠ å­è¯„è®º
                if 'child' in post:
                    for child_pid in post['child']:
                        queue.append(child_pid)
        
        return " ".join(all_texts)
    
    def get_enhanced_text(self, row_id, original_text):
        """è·å–å¢å¼ºçš„æ–‡æœ¬ï¼ˆåŸå§‹æ–‡æœ¬ + è¯„è®ºæ•°æ®ï¼‰"""
        str_id = str(row_id)
        
        # å¦‚æœIDåœ¨æ˜ å°„ä¸­ï¼Œè·å–å¯¹åº”çš„å¸–å­ID
        if str_id in self.id_post_mapping:
            post_id = self.id_post_mapping[str_id]
            comment_text = self.get_comment_tree_text(post_id)
            
            if comment_text:
                # åˆå¹¶åŸå§‹æ–‡æœ¬å’Œè¯„è®ºæ–‡æœ¬
                enhanced_text = f"{original_text} [è¯„è®ºä¸Šä¸‹æ–‡] {comment_text}"
                return enhanced_text[:5000]  # é™åˆ¶æ€»é•¿åº¦
        
        return original_text
    
    def analyze_data_distribution(self):
        """åˆ†ææ•°æ®åˆ†å¸ƒï¼Œè¯†åˆ«é—®é¢˜"""
        print(f"\n=== {self.phase}é›†æ•°æ®åˆ†å¸ƒåˆ†æ ===")
        
        # Stanceåˆ†å¸ƒ
        stance_counts = Counter(self.data['stance'])
        print(f"Stanceåˆ†å¸ƒ: {dict(stance_counts)}")
        
        # Intentåˆ†å¸ƒåˆ†æ
        intent_label_counts = [0] * len(self.intent_labels)
        intent_combinations = Counter()
        
        for idx, row in self.data.iterrows():
            intent_str = str(row['intent'])
            if pd.isna(intent_str):
                continue
                
            intent_list = intent_str.split('&')
            intent_combinations[intent_str] += 1
            
            for i, label in enumerate(self.intent_labels):
                if label in intent_list:
                    intent_label_counts[i] += 1
        
        print(f"Intentæ ‡ç­¾åˆ†å¸ƒ: {dict(zip(self.intent_labels, intent_label_counts))}")
        print(f"å‰5ä¸ªIntentç»„åˆ: {intent_combinations.most_common(5)}")
        
        # ç‰¹åˆ«å…³æ³¨Psychological_fulfillment
        psych_idx = self.intent_labels.index('Psychological_fulfillment')
        psych_count = intent_label_counts[psych_idx]
        print(f"Psychological_fulfillmentæ ·æœ¬æ•°: {psych_count} (å æ¯”: {psych_count/len(self.data)*100:.2f}%)")
        
        # Fairnessåˆ†å¸ƒ
        fairness_counts = Counter(self.data['fairness'])
        print(f"Fairnessåˆ†å¸ƒ: {dict(fairness_counts)}")
        
        # Harmfulnessåˆ†å¸ƒ
        harmfulness_counts = Counter(self.data['harmfulness'])
        print(f"Harmfulnessåˆ†å¸ƒ: {dict(harmfulness_counts)}")
        
        # åˆ†æè¯„è®ºæ•°æ®è¦†ç›–æƒ…å†µ
        comment_coverage = 0
        for idx, row in self.data.iterrows():
            str_id = str(row['id'])
            if str_id in self.id_post_mapping:
                comment_coverage += 1
        
        print(f"è¯„è®ºæ•°æ®è¦†ç›–: {comment_coverage}/{len(self.data)} ({comment_coverage/len(self.data)*100:.2f}%)")
    
    def __len__(self):
        return len(self.data)
    
    def encode_intent(self, intent_str):
        """å°†å¤šæ ‡ç­¾intentç¼–ç ä¸ºäºŒè¿›åˆ¶å‘é‡"""
        if pd.isna(intent_str):
            return [0] * len(self.intent_labels)
        
        intent_list = str(intent_str).split('&')
        encoding = [0] * len(self.intent_labels)
        
        for i, label in enumerate(self.intent_labels):
            if label in intent_list:
                encoding[i] = 1
                
        return encoding
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # è·å–åŸºç¡€æ–‡æœ¬
        base_text = str(row['text']) if 'text' in row else "default text"
        
        # è·å–å¢å¼ºæ–‡æœ¬ï¼ˆåŒ…å«è¯„è®ºï¼‰
        enhanced_text = self.get_enhanced_text(row['id'], base_text)
        
        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            enhanced_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # è·å–æ ‡ç­¾ - æ·»åŠ å®¹é”™å¤„ç†
        stance = self.stance_map.get(str(row['stance']), 1)  # é»˜è®¤Neither
        fairness = self.fairness_map.get(str(row['fairness']), 0)  # é»˜è®¤Tinted
        harmfulness = self.harmfulness_map.get(str(row['harmfulness']), 0)  # é»˜è®¤Harmful
        intent = self.encode_intent(row['intent'])
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'stance': torch.tensor(stance, dtype=torch.long),
            'intent': torch.tensor(intent, dtype=torch.float),
            'fairness': torch.tensor(fairness, dtype=torch.long),
            'harmfulness': torch.tensor(harmfulness, dtype=torch.long),
            'text_length': torch.tensor(len(enhanced_text), dtype=torch.long)
        }

class OptimizedMultiScaleFeatureExtractor(nn.Module):
    """ä¼˜åŒ–ç‰ˆå¤šå°ºåº¦ç‰¹å¾æå–å™¨"""
    
    def __init__(self, input_dim: int = 768, output_dim: int = 256, dropout: float = 0.1):
        super(OptimizedMultiScaleFeatureExtractor, self).__init__()
        
        # å¤šå°ºåº¦å·ç§¯å±‚
        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=1, padding=0)   # å•è¯è¯­ä¹‰
        self.conv2 = nn.Conv1d(input_dim, 64, kernel_size=2, padding=1)   # äºŒå…ƒå…³ç³»
        self.conv3 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)   # ä¸‰å…ƒçŸ­è¯­
        self.conv4 = nn.Conv1d(input_dim, 64, kernel_size=5, padding=2)   # çŸ­å¥æ¨¡å¼
        self.conv5 = nn.Conv1d(input_dim, 64, kernel_size=7, padding=3)   # é•¿è·ç¦»ä¾èµ–
        
        # æ®‹å·®è¿æ¥
        self.residual = nn.Sequential(
            nn.Conv1d(input_dim, 320, kernel_size=1),
            nn.BatchNorm1d(320)
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion = nn.Sequential(
            nn.Linear(320, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(512),
            nn.Linear(512, output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.LayerNorm(output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # è½¬ç½®ä¸ºå·ç§¯éœ€è¦çš„æ ¼å¼ [batch_size, hidden_size, seq_len]
        x_input = x.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯
        feat1 = F.relu(self.conv1(x_input))
        feat2 = F.relu(self.conv2(x_input))
        feat3 = F.relu(self.conv3(x_input))
        feat4 = F.relu(self.conv4(x_input))
        feat5 = F.relu(self.conv5(x_input))
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pool1 = F.adaptive_avg_pool1d(feat1, 1).squeeze(-1)
        pool2 = F.adaptive_avg_pool1d(feat2, 1).squeeze(-1)
        pool3 = F.adaptive_avg_pool1d(feat3, 1).squeeze(-1)
        pool4 = F.adaptive_avg_pool1d(feat4, 1).squeeze(-1)
        pool5 = F.adaptive_avg_pool1d(feat5, 1).squeeze(-1)
        
        # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾ + æ®‹å·®è¿æ¥
        multi_scale_features = torch.cat([pool1, pool2, pool3, pool4, pool5], dim=1)
        residual_features = F.adaptive_avg_pool1d(self.residual(x_input), 1).squeeze(-1)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion(multi_scale_features + residual_features)
        
        return fused_features

class EnhancedDifferentiatedFeatureExtractor(nn.Module):
    """å¢å¼ºç‰ˆå·®å¼‚åŒ–ç‰¹å¾æå–å™¨"""
    
    def __init__(self, perspective_type: str, input_dim: int = 768, output_dim: int = 256, dropout: float = 0.1):
        super(EnhancedDifferentiatedFeatureExtractor, self).__init__()
        self.perspective_type = perspective_type
        
        if perspective_type == "belief":
            # ä¿¡å¿µè§†è§’ï¼šæ›´æ·±ç½‘ç»œ + è‡ªæ³¨æ„åŠ›
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 512),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(512, 384),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(384, output_dim),
            )
            self.attention = nn.MultiheadAttention(output_dim, 4, dropout=dropout)
            
        elif perspective_type == "desire":
            # æ¬²æœ›è§†è§’ï¼šä¸­ç­‰æ·±åº¦ + é€šé“æ³¨æ„åŠ›
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 448),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(448, output_dim),
            )
            self.channel_attention = nn.Sequential(
                nn.Linear(output_dim, output_dim // 4),
                nn.ReLU(),
                nn.Linear(output_dim // 4, output_dim),
                nn.Sigmoid()
            )
        else:  # plan
            # è®¡åˆ’è§†è§’ï¼šæ›´æµ…ç½‘ç»œ + æ®‹å·®è¿æ¥
            self.feature_net = nn.Sequential(
                nn.Linear(input_dim, 320),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(320, output_dim),
            )
            self.residual = nn.Linear(input_dim, output_dim)
        
        self.layer_norm = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.perspective_type == "belief":
            features = self.feature_net(x)
            # è‡ªæ³¨æ„åŠ›å¢å¼º
            features_attn = features.unsqueeze(1)
            attended, _ = self.attention(features_attn, features_attn, features_attn)
            features = features + attended.squeeze(1)
            
        elif self.perspective_type == "desire":
            features = self.feature_net(x)
            # é€šé“æ³¨æ„åŠ›
            attention_weights = self.channel_attention(features)
            features = features * attention_weights
            
        else:  # plan
            base_features = self.feature_net(x)
            residual_features = self.residual(x)
            features = base_features + residual_features
        
        features = self.layer_norm(features)
        return self.dropout(features)

class CorrelationEnhancedIntentClassifier(nn.Module):
    """ç›¸å…³æ€§å¢å¼ºçš„æ„å›¾åˆ†ç±»å™¨ - ä¸“é—¨è§£å†³Psychological_fulfillmenté—®é¢˜"""
    
    def __init__(self, feature_dim: int = 256, num_intents: int = 4, dropout: float = 0.2):
        super(CorrelationEnhancedIntentClassifier, self).__init__()
        
        # ä¸ºPsychological_fulfillmentåˆ›å»ºä¸“ç”¨è·¯å¾„
        self.psych_special_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),  # æ›´å°‘çš„dropout
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(64, 1)
        )
        
        # å…¶ä»–æ„å›¾çš„å…±äº«ç½‘ç»œ
        self.shared_net = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(256),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_intents - 1)  # é™¤äº†Psychological_fulfillment
        )
        
        # æ ‡ç­¾ç›¸å…³æ€§çŸ©é˜µ - å¯å­¦ä¹ å‚æ•°
        self.label_correlation = nn.Parameter(torch.eye(num_intents))
        
        # æ³¨æ„åŠ›æœºåˆ¶æ•æ‰æ ‡ç­¾é—´å…³ç³»
        self.attention = nn.MultiheadAttention(
            embed_dim=num_intents, 
            num_heads=2,
            dropout=dropout,
            batch_first=True
        )
        
        self.num_intents = num_intents
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)
        
        # Psychological_fulfillmentä¸“ç”¨é¢„æµ‹
        psych_logit = self.psych_special_head(x)  # [batch_size, 1]
        
        # å…¶ä»–æ„å›¾çš„å…±äº«é¢„æµ‹
        other_logits = self.shared_net(x)  # [batch_size, num_intents-1]
        
        # åˆå¹¶æ‰€æœ‰logits
        intent_logits = torch.cat([other_logits[:, :1], psych_logit, other_logits[:, 1:]], dim=1)
        
        # åº”ç”¨æ ‡ç­¾ç›¸å…³æ€§çŸ©é˜µ
        correlated_logits = torch.matmul(intent_logits, self.label_correlation)
        
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥å»ºæ¨¡æ ‡ç­¾å…³ç³»
        logits_reshaped = correlated_logits.unsqueeze(1)  # [batch_size, 1, num_intents]
        attended_logits, _ = self.attention(logits_reshaped, logits_reshaped, logits_reshaped)
        attended_logits = attended_logits.squeeze(1)
        
        # æ®‹å·®è¿æ¥
        final_logits = correlated_logits + 0.3 * attended_logits
        
        return final_logits

class EnhancedFairnessClassifier(nn.Module):
    """å¢å¼ºç‰ˆå…¬å¹³æ€§åˆ†ç±»å™¨"""
    
    def __init__(self, feature_dim: int = 256, dropout: float = 0.2):
        super(EnhancedFairnessClassifier, self).__init__()
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„æ¥æ•æ‰å…¬å¹³æ€§ç‰¹å¾
        self.fairness_net = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(64, 2)  # Tinted, Fairness
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶èšç„¦å…³é”®ç‰¹å¾
        self.attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾
        x_reshaped = x.unsqueeze(1)  # [batch_size, 1, feature_dim]
        attended_x, _ = self.attention(x_reshaped, x_reshaped, x_reshaped)
        attended_x = attended_x.squeeze(1)
        
        # æ®‹å·®è¿æ¥
        enhanced_features = x + 0.2 * attended_x
        
        # åˆ†ç±»
        fairness_logits = self.fairness_net(enhanced_features)
        
        return fairness_logits

class AdvancedGatedIntentAggregator(nn.Module):
    """é«˜çº§é—¨æ§æ„å›¾èšåˆå™¨"""
    
    def __init__(self, feature_dim: int = 256, num_intents: int = 4, dropout: float = 0.1):
        super(AdvancedGatedIntentAggregator, self).__init__()
        
        # å¤šå±‚æ¬¡é—¨æ§æœºåˆ¶
        self.gate_network = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, 3),  # 3ä¸ªè§†è§’çš„æƒé‡
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion_network = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(feature_dim * 2),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # ä½¿ç”¨ç›¸å…³æ€§å¢å¼ºçš„æ„å›¾åˆ†ç±»å™¨
        self.intent_classifier = CorrelationEnhancedIntentClassifier(feature_dim, num_intents, dropout)
        
    def forward(self, belief_features: torch.Tensor, desire_features: torch.Tensor, 
                plan_features: torch.Tensor) -> Dict[str, torch.Tensor]:
        
        # æ‹¼æ¥æ‰€æœ‰è§†è§’ç‰¹å¾
        concatenated = torch.cat([belief_features, desire_features, plan_features], dim=1)
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate_weights = self.gate_network(concatenated)  # [batch_size, 3]
        
        # åº”ç”¨é—¨æ§æƒé‡çš„åŠ æƒèåˆ
        weighted_belief = belief_features * gate_weights[:, 0:1]
        weighted_desire = desire_features * gate_weights[:, 1:2]
        weighted_plan = plan_features * gate_weights[:, 2:3]
        
        # æ‹¼æ¥åŠ æƒç‰¹å¾
        weighted_concatenated = torch.cat([weighted_belief, weighted_desire, weighted_plan], dim=1)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion_network(weighted_concatenated)
        
        # æ„å›¾åˆ†ç±»
        intent_logits = self.intent_classifier(fused_features)
        
        return {
            'intent_logits': intent_logits,
            'fused_features': fused_features,
            'gate_weights': gate_weights
        }

class FourteenDMINTModel(nn.Module):
    """Fourteenç‰ˆDMINTæ¨¡å‹ - èåˆè¯„è®ºæ•°æ®å’Œæ‰€æœ‰ä¼˜åŒ–"""
    
    def __init__(self, bert_model, hidden_size: int = 768, feature_dim: int = 256, dropout: float = 0.3):
        super(FourteenDMINTModel, self).__init__()
        
        self.bert = bert_model
        
        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆå¤šå°ºåº¦ç‰¹å¾æå–å™¨
        self.multi_scale_extractor = OptimizedMultiScaleFeatureExtractor(hidden_size, feature_dim, dropout)
        
        # ä½¿ç”¨å¢å¼ºç‰ˆå·®å¼‚åŒ–ç‰¹å¾æå–å™¨
        self.belief_extractor = EnhancedDifferentiatedFeatureExtractor("belief", hidden_size, feature_dim, dropout)
        self.desire_extractor = EnhancedDifferentiatedFeatureExtractor("desire", hidden_size, feature_dim, dropout)
        self.plan_extractor = EnhancedDifferentiatedFeatureExtractor("plan", hidden_size, feature_dim, dropout)
        
        # ä½¿ç”¨é«˜çº§é—¨æ§æ„å›¾èšåˆå™¨
        self.intent_aggregator = AdvancedGatedIntentAggregator(feature_dim, 4, dropout)
        
        # ä»»åŠ¡ç‰¹å®šçš„åˆ†ç±»å™¨
        self.stance_classifier = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(256),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3)  # Against, Neither, Favor
        )
        
        self.harmfulness_classifier = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(256),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)  # Harmful, Unharmful
        )
        
        # ä½¿ç”¨å¢å¼ºç‰ˆå…¬å¹³æ€§åˆ†ç±»å™¨
        self.fairness_classifier = EnhancedFairnessClassifier(feature_dim, dropout)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        # BERTç‰¹å¾æå–
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = bert_output.last_hidden_state
        cls_features = bert_output.pooler_output
        
        # å¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆåŸºäºåºåˆ—è¾“å‡ºï¼‰
        multi_scale_features = self.multi_scale_extractor(sequence_output)
        
        # ä¸‰ä¸ªè§†è§’çš„ç‰¹å¾æå–ï¼ˆåŸºäºCLSç‰¹å¾ï¼‰
        belief_features = self.belief_extractor(cls_features)
        desire_features = self.desire_extractor(cls_features)
        plan_features = self.plan_extractor(cls_features)
        
        # åº”ç”¨dropout
        belief_features = self.dropout(belief_features)
        desire_features = self.dropout(desire_features)
        plan_features = self.dropout(plan_features)
        multi_scale_features = self.dropout(multi_scale_features)
        
        # é—¨æ§æ„å›¾èšåˆ
        intent_output = self.intent_aggregator(belief_features, desire_features, plan_features)
        fused_features = intent_output['fused_features']
        intent_logits = intent_output['intent_logits']
        
        # ä»»åŠ¡åˆ†ç±» - ç»“åˆå¤šå°ºåº¦ç‰¹å¾å’Œè§†è§’ç‰¹å¾
        stance_input = torch.cat([multi_scale_features, belief_features], dim=1)
        stance_logits = self.stance_classifier(stance_input)
        
        harmfulness_input = torch.cat([multi_scale_features, plan_features], dim=1)
        harmfulness_logits = self.harmfulness_classifier(harmfulness_input)
        
        fairness_logits = self.fairness_classifier(fused_features)
        
        return {
            'stance': stance_logits,
            'intent': intent_logits,
            'harmfulness': harmfulness_logits,
            'fairness': fairness_logits,
            'belief_features': belief_features,
            'desire_features': desire_features,
            'plan_features': plan_features,
            'multi_scale_features': multi_scale_features,
            'gate_weights': intent_output['gate_weights']
        }

def compute_enhanced_class_weights(train_df):
    """è®¡ç®—å¢å¼ºçš„ç±»åˆ«æƒé‡ï¼Œç‰¹åˆ«ä¼˜åŒ–Psychological_fulfillment"""
    intent_labels = ['Political_interest', 'Economic_interest', 
                    'Psychological_fulfillment', 'Public_interest']
    
    # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾çš„å‡ºç°æ¬¡æ•°
    label_counts = [0] * len(intent_labels)
    total_samples = len(train_df)
    
    for idx, row in train_df.iterrows():
        intent_str = str(row['intent'])
        if pd.isna(intent_str):
            continue
            
        intent_list = intent_str.split('&')
        for i, label in enumerate(intent_labels):
            if label in intent_list:
                label_counts[i] += 1
    
    print(f"Intentæ ‡ç­¾åˆ†å¸ƒ: {dict(zip(intent_labels, label_counts))}")
    
    # ç‰¹åˆ«å¤„ç†Psychological_fulfillment
    psych_idx = intent_labels.index('Psychological_fulfillment')
    psych_count = label_counts[psych_idx]
    
    # åŠ¨æ€æƒé‡è°ƒæ•´ç­–ç•¥
    base_weights = []
    for i, count in enumerate(label_counts):
        if count > 0:
            if i == psych_idx and psych_count < 500:  # Psychological_fulfillmentæ ·æœ¬è¿‡å°‘
                # ç»™äºˆæ›´é«˜çš„æƒé‡
                weight = total_samples / (len(intent_labels) * count) * 3.0
                print(f"Psychological_fulfillmentæ ·æœ¬è¿‡å°‘({psych_count})ï¼Œåº”ç”¨3å€æƒé‡: {weight:.2f}")
            else:
                weight = total_samples / (len(intent_labels) * count)
        else:
            weight = 1.0
        base_weights.append(weight)
    
    print(f"å¢å¼ºåIntentç±»åˆ«æƒé‡: {dict(zip(intent_labels, [f'{w:.2f}' for w in base_weights]))}")
    
    return torch.tensor(base_weights, dtype=torch.float)

def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ŒåŒ…å«è¯„è®ºæ•°æ®"""
    data_dir = "data/"
    
    # åŠ è½½CSVæ•°æ®
    train_df = pd.read_csv(os.path.join(data_dir, "train.csv"), sep=',', header=None,
                          names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    val_df = pd.read_csv(os.path.join(data_dir, "val.csv"), sep=',', header=None,
                        names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    test_df = pd.read_csv(os.path.join(data_dir, "test1.csv"), sep=',', header=None,
                         names=['id', 'stance', 'intent', 'fairness', 'harmfulness'])
    
    # åŠ è½½JSONæ•°æ®å¹¶æ•´åˆæ–‡æœ¬
    def load_json_data():
        topics_path = os.path.join(data_dir, "news_topic1.json")
        docs_path = os.path.join(data_dir, "news_docs.json")
        
        topics_data = {}
        docs_data = {}
        
        if os.path.exists(topics_path):
            with open(topics_path, 'r', encoding='utf-8') as f:
                topics_data = json.load(f)
        
        if os.path.exists(docs_path):
            with open(docs_path, 'r', encoding='utf-8') as f:
                docs_data = json.load(f)
        
        return topics_data, docs_data
    
    topics_data, docs_data = load_json_data()
    
    def get_base_text_content(row_id):
        """è·å–åŸºç¡€æ–‡æœ¬å†…å®¹ï¼ˆä¸åŒ…å«è¯„è®ºï¼‰"""
        str_id = str(row_id)
        topic_text = topics_data.get(str_id, "unknown topic")
        
        # å¦‚æœæœ‰æ–‡æ¡£æ•°æ®ï¼Œä½¿ç”¨æ–‡æ¡£å†…å®¹ï¼›å¦åˆ™åªä½¿ç”¨æ ‡é¢˜
        doc_content = ""
        if docs_data and str_id in docs_data:
            doc_content = docs_data.get(str_id, {}).get("content", "")
        
        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè‡³å°‘ä½¿ç”¨æ ‡é¢˜
        if not doc_content.strip():
            return topic_text
        else:
            return f"{topic_text} {doc_content}"
    
    # ä¸ºæ•°æ®æ¡†æ·»åŠ åŸºç¡€æ–‡æœ¬åˆ—ï¼ˆåç»­ä¼šåœ¨EnhancedTextDatasetä¸­å¢å¼ºï¼‰
    train_df['text'] = train_df['id'].apply(get_base_text_content)
    val_df['text'] = val_df['id'].apply(get_base_text_content)
    test_df['text'] = test_df['id'].apply(get_base_text_content)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_df)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")
    
    return train_df, val_df, test_df

class FourteenDMINTTrainer:
    """Fourteenç‰ˆè®­ç»ƒå™¨ - é›†æˆè¯„è®ºæ•°æ®å’Œæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥"""
    
    def __init__(self, model, train_loader, val_loader, device, num_epochs=10, intent_class_weights=None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.num_epochs = num_epochs
        
        # æŸå¤±å‡½æ•° - ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨å¸¦æƒé‡çš„BCE
        self.criterion_stance = nn.CrossEntropyLoss()
        
        # ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨å¸¦æƒé‡çš„BCEWithLogitsLoss
        if intent_class_weights is not None:
            self.criterion_intent = nn.BCEWithLogitsLoss(pos_weight=intent_class_weights.to(device))
            print(f"ä½¿ç”¨åŠ æƒçš„å¤šæ ‡ç­¾æŸå¤±å‡½æ•°ï¼Œæƒé‡: {intent_class_weights}")
        else:
            self.criterion_intent = nn.BCEWithLogitsLoss()
            
        self.criterion_harmfulness = nn.CrossEntropyLoss()
        self.criterion_fairness = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=2e-5,
            weight_decay=0.01
        )
        
        # ä½¿ç”¨WarmupPolyå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = WarmupPolyLR(
            self.optimizer,
            total_epochs=num_epochs,
            warmup_epochs=2,
            poly_exp=0.9,
            min_lr=1e-6,
            base_lr=2e-5
        )
        
        self.best_val_loss = float('inf')
        self.best_intent_f1 = 0.0
        self.best_psych_f1 = 0.0
        self.patience = 8
        self.counter = 0
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [], 'lr': [], 'stance_acc': [], 'intent_f1': [],
            'harmfulness_acc': [], 'fairness_acc': [], 'psych_f1': []
        }
        
    def compute_balanced_loss(self, outputs, labels):
        """è®¡ç®—å¹³è¡¡çš„å¤šä»»åŠ¡æŸå¤±ï¼Œç‰¹åˆ«å…³æ³¨Psychological_fulfillment"""
        loss_stance = self.criterion_stance(outputs['stance'], labels['stance'])
        loss_intent = self.criterion_intent(outputs['intent'], labels['intent'])
        loss_harmfulness = self.criterion_harmfulness(outputs['harmfulness'], labels['harmfulness'])
        loss_fairness = self.criterion_fairness(outputs['fairness'], labels['fairness'])
        
        # åŠ¨æ€æƒé‡è°ƒæ•´ - ç‰¹åˆ«å…³æ³¨intentå’Œfairness
        total_loss = (1.2 * loss_stance + 2.5 * loss_intent +  # é™ä½intentæƒé‡
                     1.2 * loss_harmfulness + 1.8 * loss_fairness)  # æé«˜fairnessæƒé‡
        
        return total_loss, {
            'stance': loss_stance.item(),
            'intent': loss_intent.item(),
            'harmfulness': loss_harmfulness.item(),
            'fairness': loss_fairness.item()
        }
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_batches = len(self.train_loader)
        
        epoch_start_time = time.time()
        
        for batch_idx, batch in enumerate(self.train_loader):
            batch_start_time = time.time()
            
            self.optimizer.zero_grad()
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            stance_labels = batch['stance'].to(self.device)
            intent_labels = batch['intent'].to(self.device)
            harmfulness_labels = batch['harmfulness'].to(self.device)
            fairness_labels = batch['fairness'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids, attention_mask)
            
            # è®¡ç®—å¹³è¡¡çš„å¤šä»»åŠ¡æŸå¤±
            labels = {
                'stance': stance_labels,
                'intent': intent_labels,
                'harmfulness': harmfulness_labels,
                'fairness': fairness_labels
            }
            total_loss_batch, task_losses = self.compute_balanced_loss(outputs, labels)
            
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            
            # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if batch_idx % 10 == 0:
                progress = (batch_idx / total_batches) * 100
                batch_time = time.time() - batch_start_time
                current_lr = self.optimizer.param_groups[0]['lr']
                
                print(f'  Batch {batch_idx}/{total_batches} ({progress:.1f}%), '
                      f'Loss: {total_loss_batch.item():.4f}, '
                      f'LR: {current_lr:.2e}, '
                      f'Batch Time: {batch_time:.2f}s')
        
        # æ›´æ–°å­¦ä¹ ç‡
        current_lr = self.scheduler.step()
        
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / total_batches
        return avg_loss, epoch_time, current_lr
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        total_batches = len(self.val_loader)
        
        all_predictions = {'stance': [], 'intent': [], 'harmfulness': [], 'fairness': []}
        all_labels = {'stance': [], 'intent': [], 'harmfulness': [], 'fairness': []}
        
        with torch.no_grad():
            for batch in self.val_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                stance_labels = batch['stance'].to(self.device)
                intent_labels = batch['intent'].to(self.device)
                harmfulness_labels = batch['harmfulness'].to(self.device)
                fairness_labels = batch['fairness'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids, attention_mask)
                
                # è®¡ç®—æŸå¤±
                labels = {
                    'stance': stance_labels,
                    'intent': intent_labels,
                    'harmfulness': harmfulness_labels,
                    'fairness': fairness_labels
                }
                total_loss_batch, _ = self.compute_balanced_loss(outputs, labels)
                total_loss += total_loss_batch.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions['stance'].extend(torch.argmax(outputs['stance'], 1).cpu().numpy())
                all_labels['stance'].extend(stance_labels.cpu().numpy())
                
                all_predictions['harmfulness'].extend(torch.argmax(outputs['harmfulness'], 1).cpu().numpy())
                all_labels['harmfulness'].extend(harmfulness_labels.cpu().numpy())
                
                all_predictions['fairness'].extend(torch.argmax(outputs['fairness'], 1).cpu().numpy())
                all_labels['fairness'].extend(fairness_labels.cpu().numpy())
                
                # Intentå¤šæ ‡ç­¾åˆ†ç±» - ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
                intent_probs = torch.sigmoid(outputs['intent'])
                intent_preds = (intent_probs > 0.3).float().cpu().numpy()
                all_predictions['intent'].extend(intent_preds)
                all_labels['intent'].extend(intent_labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for task in ['stance', 'harmfulness', 'fairness']:
            if len(all_labels[task]) > 0:
                metrics[f'{task}_accuracy'] = accuracy_score(all_labels[task], all_predictions[task])
                metrics[f'{task}_f1'] = f1_score(all_labels[task], all_predictions[task], average='weighted')
            else:
                metrics[f'{task}_accuracy'] = 0.0
                metrics[f'{task}_f1'] = 0.0
        
        # Intentçš„å¤šæ ‡ç­¾æŒ‡æ ‡
        try:
            intent_labels_array = np.array(all_labels['intent'])
            intent_predictions_array = np.array(all_predictions['intent'])
            
            # ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡
            intent_accuracy = np.mean(np.all(
                intent_predictions_array == intent_labels_array, axis=1
            ))
            metrics['intent_exact_match'] = float(intent_accuracy)
            
            # è®¡ç®—macro F1
            metrics['intent_macro_f1'] = f1_score(
                intent_labels_array, 
                intent_predictions_array, 
                average='macro',
                zero_division=0
            )
            
            # è®¡ç®—æ¯ä¸ªintentæ ‡ç­¾çš„F1
            intent_f1_scores = f1_score(intent_labels_array, intent_predictions_array, average=None, zero_division=0)
            intent_labels_names = ['Political', 'Economic', 'Psychological', 'Public']
            for i, label in enumerate(intent_labels_names):
                metrics[f'intent_{label}_f1'] = float(intent_f1_scores[i])
                
        except Exception as e:
            print(f"è®¡ç®—intentæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            metrics['intent_exact_match'] = 0.0
            metrics['intent_macro_f1'] = 0.0
            for label in ['Political', 'Economic', 'Psychological', 'Public']:
                metrics[f'intent_{label}_f1'] = 0.0
        
        avg_loss = total_loss / total_batches
        return avg_loss, metrics
        
    def train(self):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹Fourteenç‰ˆDMINTæ¨¡å‹è®­ç»ƒ...")
        total_start_time = time.time()
        
        for epoch in range(self.num_epochs):
            print(f"\nEpoch {epoch+1}/{self.num_epochs}")
            print("-" * 50)
            
            train_loss, epoch_time, current_lr = self.train_epoch(epoch + 1)
            val_loss, val_metrics = self.validate()
            
            # æ›´æ–°è®­ç»ƒå†å²
            self.train_history['loss'].append(train_loss)
            self.train_history['lr'].append(current_lr)
            self.train_history['stance_acc'].append(val_metrics['stance_accuracy'])
            self.train_history['intent_f1'].append(val_metrics['intent_macro_f1'])
            self.train_history['harmfulness_acc'].append(val_metrics['harmfulness_accuracy'])
            self.train_history['fairness_acc'].append(val_metrics['fairness_accuracy'])
            self.train_history['psych_f1'].append(val_metrics['intent_Psychological_f1'])
            
            # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
            total_time = time.time() - total_start_time
            hours = int(total_time // 3600)
            minutes = int((total_time % 3600) // 60)
            
            print(f"\nEpoch {epoch+1} å®Œæˆ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"  Epochæ—¶é—´: {epoch_time/60:.1f}åˆ†é’Ÿ")
            print(f"  æ€»è®­ç»ƒæ—¶é—´: {hours}å°æ—¶{minutes}åˆ†é’Ÿ")
            print(f"  éªŒè¯æŒ‡æ ‡:")
            for metric, value in val_metrics.items():
                print(f"    {metric}: {value:.4f}")
            
            # æ—©åœæœºåˆ¶ - åŸºäºPsychological_fulfillmentå’Œæ•´ä½“intent F1
            current_intent_f1 = val_metrics['intent_macro_f1']
            current_psych_f1 = val_metrics['intent_Psychological_f1']
            
            improvement = False
            if current_intent_f1 > self.best_intent_f1 + 0.005:
                self.best_intent_f1 = current_intent_f1
                improvement = True
                
            if current_psych_f1 > self.best_psych_f1 + 0.01:
                self.best_psych_f1 = current_psych_f1
                improvement = True
                print(f"ğŸ‰ Psychological_fulfillment F1æå‡åˆ°: {current_psych_f1:.4f}")
            
            if improvement:
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler,
                    'epoch': epoch,
                    'best_intent_f1': self.best_intent_f1,
                    'best_psych_f1': self.best_psych_f1,
                    'train_history': self.train_history
                }, 'outputs/fourteen_dmint_model.pth')
                print("âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹!")
                self.counter = 0
            else:
                self.counter += 1
                if self.counter >= self.patience:
                    print(f"âš  æ—©åœ: {self.patience} ä¸ªepochæœªæ˜¾è‘—æå‡")
                    break
            
            print("=" * 60)
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_df = pd.DataFrame(self.train_history)
        history_df.to_csv('outputs/fourteen_training_history.csv', index=False)
        print("è®­ç»ƒå†å²å·²ä¿å­˜åˆ° outputs/fourteen_training_history.csv")

def main():
    """ä¸»å‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
        bert_model = BertModel.from_pretrained(MODEL_PATH)
        print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # åŠ è½½æ•°æ®
        train_df, val_df, test_df = load_and_preprocess_data()
        
        # è®¡ç®—å¢å¼ºçš„ç±»åˆ«æƒé‡
        intent_class_weights = compute_enhanced_class_weights(train_df)
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆåŒ…å«è¯„è®ºæ•°æ®ï¼‰
        train_dataset = EnhancedTextDataset(train_df, tokenizer, phase="è®­ç»ƒ")
        val_dataset = EnhancedTextDataset(val_df, tokenizer, phase="éªŒè¯")
        test_dataset = EnhancedTextDataset(test_df, tokenizer, phase="æµ‹è¯•")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)
        
        print(f"è®­ç»ƒé›†batchæ•°é‡: {len(train_loader)}")
        print(f"éªŒè¯é›†batchæ•°é‡: {len(val_loader)}")
        print(f"æµ‹è¯•é›†batchæ•°é‡: {len(test_loader)}")
        
        # åˆ›å»ºFourteenç‰ˆDMINTæ¨¡å‹
        model = FourteenDMINTModel(bert_model)
        model = model.to(device)
        print("Fourteenç‰ˆDMINTæ¨¡å‹åˆ›å»ºæˆåŠŸ!")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs('outputs', exist_ok=True)
        
        # è®­ç»ƒæ¨¡å‹
        trainer = FourteenDMINTTrainer(
            model, train_loader, val_loader, device, 
            num_epochs=10,
            intent_class_weights=intent_class_weights
        )
        trainer.train()
        
        print("\nè®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜åˆ° outputs/fourteen_dmint_model.pth")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    
    import sys
    from datetime import datetime
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"training_log_{timestamp}.txt")
    
    # é‡å®šå‘æ ‡å‡†è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
    class Tee:
        def __init__(self, *files):
            self.files = files
        
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()  # ç¡®ä¿åŠæ—¶å†™å…¥
        
        def flush(self):
            for f in self.files:
                f.flush()
    
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    log_f = open(log_file, 'w', encoding='utf-8')
    
    # ä¿å­˜åŸå§‹æ ‡å‡†è¾“å‡º
    original_stdout = sys.stdout
    
    try:
        # é‡å®šå‘æ ‡å‡†è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
        sys.stdout = Tee(original_stdout, log_f)
        
        print(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        print("=" * 60)
        
        # è¿è¡Œä¸»å‡½æ•°
        main()
        
        print("=" * 60)
        print(f"è®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¢å¤æ ‡å‡†è¾“å‡ºå¹¶å…³é—­æ–‡ä»¶
        sys.stdout = original_stdout
        log_f.close()
        print(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")