"""
DMINTæ¨¡å‹éªŒè¯æµ‹è¯•ä»£ç 
ç”¨äºéªŒè¯æ¨¡å‹è®­ç»ƒæµç¨‹çš„å„ä¸ªç¯èŠ‚
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import time
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertModel
import sys

# å¯¼å…¥ä¸»æ¨¡å‹æ–‡ä»¶ä¸­çš„ç»„ä»¶
from ten import EnhancedDMINTModel, TextDataset, ImprovedDMINTTrainer

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œè®­ç»ƒæˆ–æ¨ç†
from ten import (
    TextDataset, EnhancedDMINTModel, ImprovedDMINTTrainer,
    load_and_preprocess_data, compute_intent_class_weights, safe_intent_metrics
)

class TrainingValidator:
    """è®­ç»ƒéªŒè¯å™¨ - å…¨é¢æ£€æµ‹è®­ç»ƒæµç¨‹çš„æ¯ä¸ªç¯èŠ‚"""
    
    def __init__(self):
        self.results = {}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def validate_data_loading(self):
        """éªŒè¯æ•°æ®åŠ è½½"""
        print("=" * 60)
        print("1. éªŒè¯æ•°æ®åŠ è½½")
        print("=" * 60)
        
        try:
            train_df, val_df, test_df = load_and_preprocess_data()
            
            if train_df is None or val_df is None or test_df is None:
                print("âœ— æ•°æ®åŠ è½½å¤±è´¥ - è¿”å›äº†None")
                return False
            
            print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"  è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
            print(f"  éªŒè¯é›†: {len(val_df)} æ ·æœ¬") 
            print(f"  æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬")
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            required_columns = ['id', 'stance', 'intent', 'fairness', 'harmfulness', 'text']
            for df_name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
                missing_columns = [col for col in required_columns if col not in df.columns]
                if missing_columns:
                    print(f"âœ— {df_name}ç¼ºå°‘åˆ—: {missing_columns}")
                    return False
            
            # æ£€æŸ¥æ ‡ç­¾æ•°æ®
            for df_name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
                null_counts = df[['stance', 'intent', 'fairness', 'harmfulness']].isnull().sum()
                if null_counts.sum() > 0:
                    print(f"âœ— {df_name}æœ‰ç¼ºå¤±æ ‡ç­¾: {dict(null_counts)}")
                    return False
            
            # æ£€æŸ¥æ–‡æœ¬æ•°æ®
            for df_name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
                text_lengths = df['text'].str.len()
                if text_lengths.min() == 0:
                    print(f"âœ— {df_name}æœ‰é›¶é•¿åº¦æ–‡æœ¬")
                    return False
                print(f"  {df_name}æ–‡æœ¬é•¿åº¦ - å¹³å‡: {text_lengths.mean():.1f}, æœ€å°: {text_lengths.min()}, æœ€å¤§: {text_lengths.max()}")
            
            self.results['dataframes'] = (train_df, val_df, test_df)
            return True
            
        except Exception as e:
            print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_model_creation(self):
        """éªŒè¯æ¨¡å‹åˆ›å»º"""
        print("\n" + "=" * 60)
        print("2. éªŒè¯æ¨¡å‹åˆ›å»º")
        print("=" * 60)
        
        try:
            model_path = "C:/Users/LHTBS/Desktop/check919/models/chinese-roberta-wwm-ext"
            
            print("åŠ è½½tokenizerå’ŒBERTæ¨¡å‹...")
            tokenizer = BertTokenizer.from_pretrained(model_path)
            bert_model = BertModel.from_pretrained(model_path)
            
            print("åˆ›å»ºDMINTæ¨¡å‹...")
            model = EnhancedDMINTModel(bert_model)
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"  æ€»å‚æ•°: {total_params:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"  æ¨¡å‹ç»“æ„:")
            for name, module in model.named_children():
                num_params = sum(p.numel() for p in module.parameters())
                print(f"    {name}: {num_params:,} å‚æ•°")
            
            self.results['model'] = model
            self.results['tokenizer'] = tokenizer
            return True
            
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_data_processing(self):
        """éªŒè¯æ•°æ®å¤„ç†"""
        print("\n" + "=" * 60)
        print("3. éªŒè¯æ•°æ®å¤„ç†")
        print("=" * 60)
        
        try:
            if 'dataframes' not in self.results or 'tokenizer' not in self.results:
                print("âœ— éœ€è¦å…ˆå®Œæˆæ•°æ®åŠ è½½å’Œæ¨¡å‹åˆ›å»º")
                return False
            
            train_df, val_df, test_df = self.results['dataframes']
            tokenizer = self.results['tokenizer']
            
            # ä½¿ç”¨å°‘é‡æ•°æ®æµ‹è¯•
            mini_train_df = train_df.head(32)
            train_dataset = TextDataset(mini_train_df, tokenizer)
            train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
            
            print(f"âœ“ æ•°æ®å¤„ç†æˆåŠŸ")
            print(f"  æ•°æ®é›†å¤§å°: {len(train_dataset)}")
            print(f"  Batchæ•°é‡: {len(train_loader)}")
            
            # æ£€æŸ¥ä¸€ä¸ªbatchçš„æ•°æ®
            for batch in train_loader:
                print(f"  Batchæ•°æ®å½¢çŠ¶:")
                print(f"    input_ids: {batch['input_ids'].shape}")
                print(f"    attention_mask: {batch['attention_mask'].shape}")
                print(f"    stance: {batch['stance'].shape} (å€¼èŒƒå›´: {batch['stance'].min().item()}~{batch['stance'].max().item()})")
                print(f"    intent: {batch['intent'].shape} (å€¼èŒƒå›´: {batch['intent'].min().item():.2f}~{batch['intent'].max().item():.2f})")
                print(f"    fairness: {batch['fairness'].shape} (å€¼èŒƒå›´: {batch['fairness'].min().item()}~{batch['fairness'].max().item()})")
                print(f"    harmfulness: {batch['harmfulness'].shape} (å€¼èŒƒå›´: {batch['harmfulness'].min().item()}~{batch['harmfulness'].max().item()})")
                break
            
            self.results['train_loader'] = train_loader
            return True
            
        except Exception as e:
            print(f"âœ— æ•°æ®å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_forward_pass(self):
        """éªŒè¯å‰å‘ä¼ æ’­"""
        print("\n" + "=" * 60)
        print("4. éªŒè¯å‰å‘ä¼ æ’­")
        print("=" * 60)
        
        try:
            if 'model' not in self.results or 'train_loader' not in self.results:
                print("âœ— éœ€è¦å…ˆå®Œæˆæ¨¡å‹åˆ›å»ºå’Œæ•°æ®å¤„ç†")
                return False
            
            model = self.results['model']
            train_loader = self.results['train_loader']
            
            model = model.to(self.device)
            model.eval()  # ä½¿ç”¨evalæ¨¡å¼é¿å…dropoutå½±å“
            
            for batch in train_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                with torch.no_grad():
                    outputs = model(input_ids, attention_mask)
                
                print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶:")
                print(f"    stance: {outputs['stance'].shape}")
                print(f"    intent: {outputs['intent'].shape}")
                print(f"    fairness: {outputs['fairness'].shape}")
                print(f"    harmfulness: {outputs['harmfulness'].shape}")
                print(f"    gate_weights: {outputs['gate_weights'].shape}")
                
                # æ£€æŸ¥è¾“å‡ºå€¼èŒƒå›´
                print(f"  è¾“å‡ºå€¼èŒƒå›´:")
                print(f"    stance: {outputs['stance'].min().item():.4f} ~ {outputs['stance'].max().item():.4f}")
                print(f"    intent: {outputs['intent'].min().item():.4f} ~ {outputs['intent'].max().item():.4f}")
                print(f"    fairness: {outputs['fairness'].min().item():.4f} ~ {outputs['fairness'].max().item():.4f}")
                print(f"    harmfulness: {outputs['harmfulness'].min().item():.4f} ~ {outputs['harmfulness'].max().item():.4f}")
                print(f"    gate_weights: {outputs['gate_weights'].min().item():.4f} ~ {outputs['gate_weights'].max().item():.4f}")
                
                # æ£€æŸ¥gate_weightsæ˜¯å¦åˆç†ï¼ˆåº”è¯¥å’Œä¸º1ï¼‰
                gate_sums = outputs['gate_weights'].sum(dim=1)
                print(f"  gate_weightsæ¯è¡Œå’Œ: {gate_sums.min().item():.4f} ~ {gate_sums.max().item():.4f}")
                
                break
            
            self.results['model'] = model
            return True
            
        except Exception as e:
            print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_loss_calculation(self):
        """éªŒè¯æŸå¤±è®¡ç®—"""
        print("\n" + "=" * 60)
        print("5. éªŒè¯æŸå¤±è®¡ç®—")
        print("=" * 60)
        
        try:
            if 'model' not in self.results or 'train_loader' not in self.results or 'dataframes' not in self.results:
                print("âœ— éœ€è¦å…ˆå®Œæˆå‰é¢çš„éªŒè¯æ­¥éª¤")
                return False
            
            train_df, _, _ = self.results['dataframes']
            model = self.results['model']
            train_loader = self.results['train_loader']
            
            # è®¡ç®—ç±»åˆ«æƒé‡
            intent_class_weights = compute_intent_class_weights(train_df)
            
            # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä½¿ç”¨train_loaderä½œä¸ºéªŒè¯é›†è¿›è¡Œæµ‹è¯•ï¼‰
            trainer = ImprovedDMINTTrainer(
                model, train_loader, train_loader, self.device,
                num_epochs=1,
                intent_class_weights=intent_class_weights
            )
            
            # æµ‹è¯•æŸå¤±è®¡ç®—
            model.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
            
            for batch in train_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                stance_labels = batch['stance'].to(self.device)
                intent_labels = batch['intent'].to(self.device)
                harmfulness_labels = batch['harmfulness'].to(self.device)
                fairness_labels = batch['fairness'].to(self.device)
                
                outputs = model(input_ids, attention_mask)
                
                # è®¡ç®—å„ä¸ªä»»åŠ¡çš„æŸå¤±
                loss_stance = trainer.criterion_stance(outputs['stance'], stance_labels)
                loss_intent = trainer.criterion_intent(outputs['intent'], intent_labels)
                loss_harmfulness = trainer.criterion_harmfulness(outputs['harmfulness'], harmfulness_labels)
                loss_fairness = trainer.criterion_fairness(outputs['fairness'], fairness_labels)
                
                total_loss = (loss_stance + 4.0 * loss_intent + loss_harmfulness + loss_fairness)
                
                print(f"âœ“ æŸå¤±è®¡ç®—æˆåŠŸ")
                print(f"  å„ä»»åŠ¡æŸå¤±:")
                print(f"    stance: {loss_stance.item():.4f}")
                print(f"    intent: {loss_intent.item():.4f}")
                print(f"    harmfulness: {loss_harmfulness.item():.4f}")
                print(f"    fairness: {loss_fairness.item():.4f}")
                print(f"  æ€»æŸå¤±: {total_loss.item():.4f}")
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºæœ‰é™å€¼
                if not torch.isfinite(total_loss):
                    print(f"âœ— æŸå¤±å€¼ä¸ºéæœ‰é™å€¼: {total_loss.item()}")
                    return False
                
                break
            
            self.results['trainer'] = trainer
            return True
            
        except Exception as e:
            print(f"âœ— æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_backward_pass(self):
        """éªŒè¯åå‘ä¼ æ’­"""
        print("\n" + "=" * 60)
        print("6. éªŒè¯åå‘ä¼ æ’­")
        print("=" * 60)
        
        try:
            if 'model' not in self.results or 'train_loader' not in self.results or 'trainer' not in self.results:
                print("âœ— éœ€è¦å…ˆå®Œæˆå‰é¢çš„éªŒè¯æ­¥éª¤")
                return False
            
            model = self.results['model']
            train_loader = self.results['train_loader']
            trainer = self.results['trainer']
            
            model.train()
            trainer.optimizer.zero_grad()
            
            # æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ­¥éª¤
            for batch in train_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                stance_labels = batch['stance'].to(self.device)
                intent_labels = batch['intent'].to(self.device)
                harmfulness_labels = batch['harmfulness'].to(self.device)
                fairness_labels = batch['fairness'].to(self.device)
                
                outputs = model(input_ids, attention_mask)
                
                loss_stance = trainer.criterion_stance(outputs['stance'], stance_labels)
                loss_intent = trainer.criterion_intent(outputs['intent'], intent_labels)
                loss_harmfulness = trainer.criterion_harmfulness(outputs['harmfulness'], harmfulness_labels)
                loss_fairness = trainer.criterion_fairness(outputs['fairness'], fairness_labels)
                
                total_loss = (loss_stance + 4.0 * loss_intent + loss_harmfulness + loss_fairness)
                total_loss.backward()
                
                # æ£€æŸ¥æ¢¯åº¦
                has_gradients = False
                gradient_norms = []
                
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        has_gradients = True
                        grad_norm = param.grad.norm().item()
                        gradient_norms.append(grad_norm)
                
                if has_gradients:
                    print(f"âœ“ åå‘ä¼ æ’­æˆåŠŸ")
                    print(f"  æ£€æµ‹åˆ°æ¢¯åº¦çš„å‚æ•°æ•°é‡: {len(gradient_norms)}")
                    print(f"  æ¢¯åº¦èŒƒæ•°èŒƒå›´: {min(gradient_norms):.6f} ~ {max(gradient_norms):.6f}")
                    
                    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºæœ‰é™å€¼
                    if not all(np.isfinite(grad_norm) for grad_norm in gradient_norms):
                        print(f"âœ— æ¢¯åº¦åŒ…å«éæœ‰é™å€¼")
                        return False
                else:
                    print(f"âœ— æœªæ£€æµ‹åˆ°æ¢¯åº¦")
                    return False
                
                # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
                trainer.optimizer.step()
                print(f"âœ“ ä¼˜åŒ–å™¨æ­¥éª¤å®Œæˆ")
                
                break
            
            return True
            
        except Exception as e:
            print(f"âœ— åå‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def validate_training_loop(self):
        """éªŒè¯å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("7. éªŒè¯å®Œæ•´è®­ç»ƒå¾ªç¯")
        print("=" * 60)
        
        try:
            if 'model' not in self.results or 'train_loader' not in self.results or 'trainer' not in self.results:
                print("âœ— éœ€è¦å…ˆå®Œæˆå‰é¢çš„éªŒè¯æ­¥éª¤")
                return False
            
            model = self.results['model']
            train_loader = self.results['train_loader']
            trainer = self.results['trainer']
            
            print("è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒepoch...")
            
            # è®°å½•åˆå§‹å‚æ•°
            initial_params = {}
            for name, param in model.named_parameters():
                if param.requires_grad:
                    initial_params[name] = param.data.clone()
            
            # è¿è¡Œä¸€ä¸ªè®­ç»ƒepoch
            train_loss, epoch_time = trainer.train_epoch(1)
            
            # æ£€æŸ¥å‚æ•°æ˜¯å¦æ›´æ–°
            params_updated = False
            for name, param in model.named_parameters():
                if param.requires_grad:
                    if not torch.equal(initial_params[name], param.data):
                        params_updated = True
                        break
            
            if params_updated:
                print(f"âœ“ è®­ç»ƒå¾ªç¯æˆåŠŸ")
                print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
                print(f"  epochæ—¶é—´: {epoch_time:.2f}ç§’")
                print(f"  æ¨¡å‹å‚æ•°å·²æ›´æ–°")
            else:
                print(f"âš  è®­ç»ƒå®Œæˆä½†å‚æ•°æœªæ›´æ–°")
            
            return True
            
        except Exception as e:
            print(f"âœ— è®­ç»ƒå¾ªç¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_complete_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("å¼€å§‹è®­ç»ƒæµç¨‹éªŒè¯...")
        print("=" * 60)
        
        validation_steps = [
            ("æ•°æ®åŠ è½½", self.validate_data_loading),
            ("æ¨¡å‹åˆ›å»º", self.validate_model_creation),
            ("æ•°æ®å¤„ç†", self.validate_data_processing),
            ("å‰å‘ä¼ æ’­", self.validate_forward_pass),
            ("æŸå¤±è®¡ç®—", self.validate_loss_calculation),
            ("åå‘ä¼ æ’­", self.validate_backward_pass),
            ("è®­ç»ƒå¾ªç¯", self.validate_training_loop),
        ]
        
        passed_steps = 0
        total_steps = len(validation_steps)
        
        for step_name, validation_func in validation_steps:
            try:
                success = validation_func()
                if success:
                    passed_steps += 1
                    print(f"âœ“ {step_name} - é€šè¿‡")
                else:
                    print(f"âœ— {step_name} - å¤±è´¥")
                    break
            except Exception as e:
                print(f"âœ— {step_name} - å¼‚å¸¸: {e}")
                break
        
        print("\n" + "=" * 60)
        print("éªŒè¯ç»“æœæ€»ç»“")
        print("=" * 60)
        print(f"é€šè¿‡æ­¥éª¤: {passed_steps}/{total_steps}")
        
        if passed_steps == total_steps:
            print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼ä»£ç å¯ä»¥æ­£å¸¸è®­ç»ƒ")
            print("\nå»ºè®®ä¸‹ä¸€æ­¥:")
            print("1. è¿è¡Œå®Œæ•´è®­ç»ƒ")
            print("2. ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’ŒæŒ‡æ ‡")
            print("3. æ£€æŸ¥éªŒè¯é›†æ€§èƒ½")
            return True
        else:
            print("âš  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
            print(f"å¤±è´¥æ­¥éª¤: {validation_steps[passed_steps][0] if passed_steps < total_steps else 'N/A'}")
            return False

def test_model_inference():
    """æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ¨¡å‹æ¨ç†")
    print("=" * 60)
    
    try:
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model_path = "C:/Users/LHTBS/Desktop/check919/models/chinese-roberta-wwm-ext"
        tokenizer = BertTokenizer.from_pretrained(model_path)
        bert_model = BertModel.from_pretrained(model_path)
        model = EnhancedDMINTModel(bert_model)
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ¨¡å‹æ¨ç†åŠŸèƒ½",
            "å¦ä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ£€æŸ¥å¤šä»»åŠ¡åˆ†ç±»æ•ˆæœ"
        ]
        
        # ç¼–ç æ–‡æœ¬
        encoding = tokenizer(
            test_texts,
            truncation=True,
            padding=True,
            max_length=256,
            return_tensors='pt'
        )
        
        # æ¨¡å‹æ¨ç†
        model.eval()
        with torch.no_grad():
            outputs = model(encoding['input_ids'], encoding['attention_mask'])
        
        print("âœ“ æ¨¡å‹æ¨ç†æˆåŠŸ")
        print("æ¨ç†ç»“æœ:")
        print(f"  Stance logitså½¢çŠ¶: {outputs['stance'].shape}")
        print(f"  Intent logitså½¢çŠ¶: {outputs['intent'].shape}")
        print(f"  Harmfulness logitså½¢çŠ¶: {outputs['harmfulness'].shape}")
        print(f"  Fairness logitså½¢çŠ¶: {outputs['fairness'].shape}")
        
        # è½¬æ¢é¢„æµ‹ç»“æœ
        stance_preds = torch.argmax(outputs['stance'], dim=1)
        intent_probs = torch.sigmoid(outputs['intent'])
        harmfulness_preds = torch.argmax(outputs['harmfulness'], dim=1)
        fairness_preds = torch.argmax(outputs['fairness'], dim=1)
        
        print(f"  Stanceé¢„æµ‹: {stance_preds.tolist()}")
        print(f"  Intentæ¦‚ç‡: {intent_probs.tolist()}")
        print(f"  Harmfulnessé¢„æµ‹: {harmfulness_preds.tolist()}")
        print(f"  Fairnessé¢„æµ‹: {fairness_preds.tolist()}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("DMINTæ¨¡å‹éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = TrainingValidator()
    
    # è¿è¡Œå®Œæ•´éªŒè¯
    validation_success = validator.run_complete_validation()
    
    if validation_success:
        # è¿è¡Œæ¨¡å‹æ¨ç†æµ‹è¯•
        test_model_inference()
        
        print("\n" + "=" * 60)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print("âœ“ æ¨¡å‹éªŒè¯é€šè¿‡")
        print("âœ“ å¯ä»¥å®‰å…¨è¿›è¡Œè®­ç»ƒ")
    else:
        print("\n" + "=" * 60)
        print("éªŒè¯å¤±è´¥")
        print("=" * 60)
        print("âš  è¯·æ ¹æ®é”™è¯¯ä¿¡æ¯ä¿®å¤é—®é¢˜åå†è¿›è¡Œè®­ç»ƒ")

if __name__ == "__main__":
    main()