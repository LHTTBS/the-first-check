#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šä»»åŠ¡æ¨¡å‹è®­ç»ƒæ—¥å¿—è§£æä¸å¯è§†åŒ–å·¥å…·
ç›´æ¥ä»txtæ—¥å¿—æå–æ•°æ®å¹¶ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
"""

import re
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys

# ==================== ä¸­æ–‡è®¾ç½® ====================
import matplotlib
font_candidates = ['Microsoft YaHei', 'SimHei', 'PingFang SC', 'STHeiti', 'Arial Unicode MS']
available_font = None
for font in font_candidates:
    try:
        matplotlib.font_manager.findfont(font)
        available_font = font
        break
    except:
        continue

if available_font:
    plt.rcParams['font.sans-serif'] = [available_font]
    print(f"âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {available_font}")
else:
    print("âš ï¸  æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½æ— æ³•æ˜¾ç¤ºä¸­æ–‡")
    
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.facecolor'] = 'white'
sns.set_style("whitegrid", {'axes.grid': True, 'grid.alpha': 0.3})

# ==================== é…ç½®åŒºåŸŸ ====================
LOG_FILE = "outputs\logs-34.txt"  # ä½ çš„txtæ–‡ä»¶åï¼Œå¯ä¿®æ”¹
OUTPUT_DIR = "outputs"  # è¾“å‡ºç›®å½•

# ==================== æ—¥å¿—è§£ææ¨¡å— ====================
def parse_training_log(txt_path):
    """
    è§£æè®­ç»ƒæ—¥å¿—txtæ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–æ•°æ®
    æ”¯æŒç§‘å­¦è®¡æ•°æ³•çš„å­¦ä¹ ç‡æ ¼å¼
    """
    print(f"ğŸ“‚ æ­£åœ¨è§£ææ—¥å¿—æ–‡ä»¶: {txt_path}")
    
    with open(txt_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰epochåˆ†å‰²
    epoch_blocks = re.split(r'=+', content)
    history = {
        "epochs": [], "train_loss": [], "val_loss": [], "learning_rate": [],
        "stance_acc": [], "stance_f1": [], "harmfulness_acc": [], "harmfulness_f1": [],
        "fairness_acc": [], "fairness_f1": [], "intent_em": [], "intent_macro_f1": [],
        "intent_political_f1": [], "intent_economic_f1": [],
        "intent_psychological_f1": [], "intent_public_f1": []
    }
    
    epoch_count = 0
    
    for block in epoch_blocks:
        if "Epoch" not in block or "å®Œæˆ:" not in block:
            continue
            
        # æå–epochç¼–å·
        epoch_match = re.search(r'Epoch (\d+)', block)
        if not epoch_match:
            continue
        
        epoch = int(epoch_match.group(1))
        
        # æå–æŒ‡æ ‡ï¼ˆæ”¯æŒç§‘å­¦è®¡æ•°æ³•ï¼‰
        patterns = {
            "train_loss": r"è®­ç»ƒæŸå¤±: ([\d.]+)",
            "val_loss": r"éªŒè¯æŸå¤±: ([\d.]+)",
            "learning_rate": r"å½“å‰å­¦ä¹ ç‡: ([\de\-\.]+)",
            "stance_acc": r"stance_accuracy: ([\d.]+)",
            "stance_f1": r"stance_f1: ([\d.]+)",
            "harmfulness_acc": r"harmfulness_accuracy: ([\d.]+)",
            "harmfulness_f1": r"harmfulness_f1: ([\d.]+)",
            "fairness_acc": r"fairness_accuracy: ([\d.]+)",
            "fairness_f1": r"fairness_f1: ([\d.]+)",
            "intent_em": r"intent_exact_match: ([\d.]+)",
            "intent_macro_f1": r"intent_macro_f1: ([\d.]+)",
            "intent_political_f1": r"intent_Political_f1: ([\d.]+)",
            "intent_economic_f1": r"intent_Economic_f1: ([\d.]+)",
            "intent_psychological_f1": r"intent_Psychological_f1: ([\d.]+)",
            "intent_public_f1": r"intent_Public_f1: ([\d.]+)"
        }
        
        epoch_data = {"epoch": epoch}
        for key, pattern in patterns.items():
            match = re.search(pattern, block)
            if match:
                epoch_data[key] = float(match.group(1))
            else:
                epoch_data[key] = 0.0
        
        # æ·»åŠ åˆ°history
        for key in history.keys():
            if key != "epochs":
                history[key].append(epoch_data[key])
        history["epochs"].append(epoch)
        epoch_count += 1
    
    if epoch_count == 0:
        print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ ¼å¼")
        return None
    
    print(f"âœ… æˆåŠŸè§£æ {epoch_count} ä¸ªepoch")
    return history

# ==================== ç»˜å›¾å‡½æ•° ====================
def plot_training_analysis(history):
    """ç”Ÿæˆ2x4å¸ƒå±€çš„åˆ†æå›¾è¡¨ï¼Œä¸¥æ ¼å¤ç°åŸå§‹å›¾è¡¨"""
    epochs = history['epochs']
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    fig.suptitle('å¤šä»»åŠ¡æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ç»¼åˆåˆ†æ', fontsize=16, fontweight='bold', y=0.995)
    
    # 1. è®­ç»ƒä¸éªŒè¯æŸå¤±ï¼ˆå·¦ä¸Šå›¾ï¼‰
    axes[0,0].plot(epochs, history['train_loss'], 'b-', linewidth=2.5, label='è®­ç»ƒæŸå¤±', marker='o', markersize=3)
    axes[0,0].plot(epochs, history['val_loss'], 'r-', linewidth=2.5, label='éªŒè¯æŸå¤±', marker='s', markersize=3)
    axes[0,0].set_title('è®­ç»ƒä¸éªŒè¯æŸå¤±', fontweight='bold', fontsize=12)
    axes[0,0].set_xlabel('Epochs', fontsize=10)
    axes[0,0].set_ylabel('Loss', fontsize=10)
    axes[0,0].legend(loc='upper right', fontsize=9)
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå³ä¸Šå›¾ï¼‰
    axes[0,1].plot(epochs, history['learning_rate'], 'g-', linewidth=2.5, marker='^', markersize=3)
    axes[0,1].set_title('å­¦ä¹ ç‡è°ƒåº¦', fontweight='bold', fontsize=12)
    axes[0,1].set_xlabel('Epochs', fontsize=10)
    axes[0,1].set_ylabel('Learning Rate', fontsize=10)
    axes[0,1].grid(True, alpha=0.3)
    axes[0,1].set_yscale('log')
    
    # 3. ç«‹åœºä»»åŠ¡ï¼ˆä¸­å·¦ï¼‰
    axes[0,2].plot(epochs, history['stance_acc'], 'c-', linewidth=2.5, label='å‡†ç¡®ç‡', marker='o', markersize=3)
    axes[0,2].plot(epochs, history['stance_f1'], 'm-', linewidth=2.5, label='F1-Score', marker='s', markersize=3)
    axes[0,2].set_title('ç«‹åœºæ£€æµ‹ä»»åŠ¡', fontweight='bold', fontsize=12)
    axes[0,2].set_xlabel('Epochs', fontsize=10)
    axes[0,2].set_ylabel('Score', fontsize=10)
    axes[0,2].legend(loc='lower right', fontsize=9)
    axes[0,2].grid(True, alpha=0.3)
    axes[0,2].set_ylim(0.6, 0.85)
    
    # 4. æœ‰å®³æ€§ä»»åŠ¡ï¼ˆä¸­å³ï¼‰
    axes[0,3].plot(epochs, history['harmfulness_acc'], 'c-', linewidth=2.5, label='å‡†ç¡®ç‡', marker='o', markersize=3)
    axes[0,3].plot(epochs, history['harmfulness_f1'], 'm-', linewidth=2.5, label='F1-Score', marker='s', markersize=3)
    axes[0,3].set_title('æœ‰å®³æ€§æ£€æµ‹ä»»åŠ¡', fontweight='bold', fontsize=12)
    axes[0,3].set_xlabel('Epochs', fontsize=10)
    axes[0,3].set_ylabel('Score', fontsize=10)
    axes[0,3].legend(loc='lower right', fontsize=9)
    axes[0,3].grid(True, alpha=0.3)
    axes[0,3].set_ylim(0.6, 0.85)
    
    # 5. å…¬å¹³æ€§ä»»åŠ¡ï¼ˆå·¦ä¸‹ï¼‰
    axes[1,0].plot(epochs, history['fairness_f1'], color='#ff7f0e', linewidth=2.5, marker='D', markersize=3)
    axes[1,0].set_title('å…¬å¹³æ€§ä»»åŠ¡ F1-Score', fontweight='bold', fontsize=12)
    axes[1,0].set_xlabel('Epochs', fontsize=10)
    axes[1,0].set_ylabel('F1 Score', fontsize=10)
    axes[1,0].grid(True, alpha=0.3)
    axes[1,0].set_ylim(0.6, 0.85)
    
    # 6. æ„å›¾è¯†åˆ«æ€»ä½“ï¼ˆä¸‹ä¸­å·¦ï¼‰
    axes[1,1].plot(epochs, history['intent_em'], 'purple', linewidth=2.5, label='Exact Match', marker='o', markersize=3)
    axes[1,1].plot(epochs, history['intent_macro_f1'], 'brown', linewidth=2.5, label='Macro F1', marker='s', markersize=3)
    axes[1,1].set_title('æ„å›¾è¯†åˆ«ä»»åŠ¡ï¼ˆæ€»ä½“ï¼‰', fontweight='bold', fontsize=12)
    axes[1,1].set_xlabel('Epochs', fontsize=10)
    axes[1,1].set_ylabel('Score', fontsize=10)
    axes[1,1].legend(loc='lower right', fontsize=9)
    axes[1,1].grid(True, alpha=0.3)
    axes[1,1].set_ylim(0.15, 0.45)
    
    # 7. æ„å›¾è¯†åˆ«å„ç±»åˆ«ï¼ˆä¸‹ä¸­å³ï¼‰
    axes[1,2].plot(epochs, history['intent_political_f1'], 'r-', linewidth=2, label='æ”¿æ²»', marker='o', markersize=2)
    axes[1,2].plot(epochs, history['intent_economic_f1'], 'g-', linewidth=2, label='ç»æµ', marker='s', markersize=2)
    axes[1,2].plot(epochs, history['intent_psychological_f1'], 'b-', linewidth=2, label='å¿ƒç†', marker='^', markersize=2)
    axes[1,2].plot(epochs, history['intent_public_f1'], 'orange', linewidth=2, label='å…¬ä¼—', marker='d', markersize=2)
    axes[1,2].set_title('æ„å›¾è¯†åˆ« - å„ç±»åˆ« F1', fontweight='bold', fontsize=12)
    axes[1,2].set_xlabel('Epochs', fontsize=10)
    axes[1,2].set_ylabel('F1 Score', fontsize=10)
    axes[1,2].legend(loc='lower right', fontsize=8, ncol=2)
    axes[1,2].grid(True, alpha=0.3)
    axes[1,2].set_ylim(0.15, 0.7)
    
    # 8. æ€§èƒ½æ€»ç»“è¡¨æ ¼ï¼ˆå³ä¸‹ï¼‰
    axes[1,3].axis('off')
    
    summary_data = [
        ['ä»»åŠ¡', 'æœ€ç»ˆå¾—åˆ†', 'æå‡å¹…åº¦'],
        ['ç«‹åœºæ£€æµ‹', f"{history['stance_f1'][-1]:.3f}", f"+{history['stance_f1'][-1] - history['stance_f1'][0]:.3f}"],
        ['æœ‰å®³æ€§æ£€æµ‹', f"{history['harmfulness_f1'][-1]:.3f}", f"+{history['harmfulness_f1'][-1] - history['harmfulness_f1'][0]:.3f}"],
        ['å…¬å¹³æ€§', f"{history['fairness_f1'][-1]:.3f}", f"+{history['fairness_f1'][-1] - history['fairness_f1'][0]:.3f}"],
        ['æ„å›¾è¯†åˆ«(EM)', f"{history['intent_em'][-1]:.3f}", f"+{history['intent_em'][-1] - history['intent_em'][0]:.3f}"],
        ['æ„å›¾è¯†åˆ«(F1)', f"{history['intent_macro_f1'][-1]:.3f}", f"+{history['intent_macro_f1'][-1] - history['intent_macro_f1'][0]:.3f}"]
    ]
    
    table = axes[1,3].table(cellText=summary_data, cellLoc='center', loc='center',
                           colWidths=[0.35, 0.25, 0.25])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.2)
    
    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(len(summary_data[0])):
        cell = table[(0, i)]
        cell.set_facecolor('#2C3E50')
        cell.set_text_props(weight='bold', color='white')
    
    axes[1,3].set_title('ä»»åŠ¡æ€§èƒ½æ€»ç»“', fontweight='bold', fontsize=12, pad=20)
    
    plt.tight_layout()
    plt.savefig('training_analysis.png', dpi=300, bbox_inches='tight', facecolor='white')
    print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜: training_analysis.png")
    plt.close()

# ==================== æ•°æ®ä¿å­˜ ====================
def save_all_data(history):
    """ä¿å­˜è§£æåçš„æ‰€æœ‰æ•°æ®"""
    Path(OUTPUT_DIR).mkdir(exist_ok=True)
    
    # CSVæ ¼å¼
    df = pd.DataFrame(history)
    df.to_csv(f"{OUTPUT_DIR}/training_history.csv", index=False, encoding='utf-8')
    
    # JSONæ ¼å¼
    with open(f"{OUTPUT_DIR}/training_history.json", 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)
    
    # æœ€ä½³æ¨¡å‹ä¿¡æ¯
    best_epoch = history['val_loss'].index(min(history['val_loss']))
    best_info = {
        'best_epoch': best_epoch + 1,
        'val_loss': history['val_loss'][best_epoch],
        'train_loss': history['train_loss'][best_epoch],
        'stance_f1': history['stance_f1'][best_epoch],
        'harmfulness_f1': history['harmfulness_f1'][best_epoch],
        'fairness_f1': history['fairness_f1'][best_epoch],
        'intent_em': history['intent_em'][best_epoch],
        'intent_macro_f1': history['intent_macro_f1'][best_epoch],
        'intent_political_f1': history['intent_political_f1'][best_epoch],
        'intent_economic_f1': history['intent_economic_f1'][best_epoch],
        'intent_psychological_f1': history['intent_psychological_f1'][best_epoch],
        'intent_public_f1': history['intent_public_f1'][best_epoch]
    }
    
    with open(f"{OUTPUT_DIR}/best_model.json", 'w') as f:
        json.dump(best_info, f, indent=2)
    
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {OUTPUT_DIR}/ ç›®å½•")
    return best_info

# ==================== åˆ†ææŠ¥å‘Š ====================
def generate_comprehensive_report(history, best_info):
    """ç”Ÿæˆç»¼åˆæ–‡å­—æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š è®­ç»ƒåˆ†ææŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ¯ è®­ç»ƒæ¦‚å†µ:")
    print(f"   æ€»è½®æ•°: {len(history['epochs'])}")
    print(f"   æœ€ä½³è½®æ¬¡: ç¬¬ {best_info['best_epoch']} è½®")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_info['val_loss']:.4f}")
    
    print(f"\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:")
    for task in ['stance_f1', 'harmfulness_f1', 'fairness_f1', 'intent_macro_f1']:
        print(f"   {task}: {history[task][-1]:.4f}")
    
    print(f"\nğŸš¨ ä¸¥é‡é—®é¢˜æ£€æµ‹:")
    zero_count = sum(1 for f1 in history['intent_economic_f1'] if f1 < 0.01)
    if zero_count > 5:
        print(f"   ğŸ”´ Economicæ„å›¾F1ä¸º0çš„è½®æ•°: {zero_count}/{len(history['epochs'])}")
        print("   â†’ å¯èƒ½åŸå› : éªŒè¯é›†æ— æ­£æ ·æœ¬/æ ‡ç­¾ç¼–ç é”™è¯¯/æŸå¤±æƒé‡å¼‚å¸¸")
    
    zero_count = sum(1 for f1 in history['intent_psychological_f1'] if f1 < 0.01)
    if zero_count > 5:
        print(f"   ğŸ”´ Psychologicalæ„å›¾F1ä¸º0çš„è½®æ•°: {zero_count}/{len(history['epochs'])}")
        print("   â†’ å¯èƒ½åŸå› : åŒä¸Š")
    
    gap = history['val_loss'][-1] - history['train_loss'][-1]
    if gap > 0.3:
        print(f"   ğŸŸ¡ è¿‡æ‹Ÿåˆé£é™©: éªŒè¯æŸå¤±æ¯”è®­ç»ƒæŸå¤±é«˜ {gap:.3f}")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    if zero_count > 5:
        print("   1. ç«‹å³æ£€æŸ¥éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ")
        print("   2. æ£€æŸ¥intentå­ä»»åŠ¡çš„æ ‡ç­¾ç¼–ç é€»è¾‘")
        print("   3. æ£€æŸ¥åŠ æƒæŸå¤±å‡½æ•°çš„æƒé‡è®¡ç®—")
    else:
        print("   æ¨¡å‹è®­ç»ƒæ­£å¸¸ï¼Œå¯ä»¥åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½")

# ==================== ä¸»å…¥å£ ====================
def main():
    """ä¸»æµç¨‹"""
    print("="*60)
    print("ğŸš€ è®­ç»ƒæ—¥å¿—åˆ†æå·¥å…·")
    print("="*60)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not Path(LOG_FILE).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {LOG_FILE}")
        print("è¯·å°†æ—¥å¿—æ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ï¼Œæˆ–ä¿®æ”¹LOG_FILEå˜é‡")
        sys.exit(1)
    
    # è§£ææ—¥å¿—
    history = parse_training_log(LOG_FILE)
    if history is None:
        sys.exit(1)
    
    # ä¿å­˜æ•°æ®
    best_info = save_all_data(history)
    
    # ç”Ÿæˆå›¾è¡¨
    plot_training_analysis(history)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_comprehensive_report(history, best_info)
    
    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*60)
    print("ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
    print("   - training_analysis.png (å›¾è¡¨)")
    print(f"   - {OUTPUT_DIR}/training_history.csv (æ•°æ®)")
    print(f"   - {OUTPUT_DIR}/training_history.json (æ•°æ®)")
    print(f"   - {OUTPUT_DIR}/best_model.json (æœ€ä½³æ¨¡å‹ä¿¡æ¯)")

if __name__ == "__main__":
    main()